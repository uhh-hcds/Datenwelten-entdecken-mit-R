{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4a8a80-bc20-4fdb-985e-061d41fa23a1",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "In dieser Übung werden wir die in der Vorlesung kennengelernten Inhalte zur Regressionsanalyse direkt in R anwenden. Dafür werden wir uns zunächst eine einfache lineare Regression ansehen. Diese können wir mit Tools von `base` oder auch direkt in einer Grafik mit `ggplot2` umsetzen. \n",
    "\n",
    "Anschließend wollen wir die Decision Trees aus der vorhergehenden Übung mit der Regressionsanalyse verbinden. Wir werden uns ein Modell bauen, welches analog wie die klassifizierenden Decision Trees funktioniert, aber mit kontinuierlichen Variablen arbeiten kann! Wir erinnern uns – in der letzten Wochen hatten wir es mit Klassifizierungsproblemen zu tun. D.h., eine Zielvariable konnte nur eine begrenzte Anzahl an Kategorien annehmen (die Pilze konnten z.B. nur `edible` oder `poisonous` sein). Jetzt lernen wir ein Modell kennen, welches für numerische, kontinuierliche Variablen funktioniert.\n",
    "\n",
    "Für die theoretischen Grundlagen der Regressionsanalyse sei an dieser Stelle wieder auf die Vorlesung verwiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f2ea4-dd9f-4bd3-a45b-478a4c644993",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(\n",
    "    tidyverse,\n",
    "    tidymodels,\n",
    "    vip,\n",
    "    rpart.plot,\n",
    "    palmerpenguins\n",
    "    )\n",
    "\n",
    "# Set the default plot width for Jupyter Notebook display\n",
    "options(repr.plot.width = 12, repr.plot.height = 8, digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeea7fe-f191-4f3a-87a0-3334356e6ab7",
   "metadata": {},
   "source": [
    "## Lineare Regression\n",
    "\n",
    "Wir erinnern uns an das Paket `palmerpenguins` aus der 2. Stunde? Über die Pinguine des Datensatzes wurden verschiedene Parameter erhoben, wie beispielsweise die Flügellänge `flipper_length_mm` und das Körpergewicht `body_mass_g`. Aus der Biologie ließe sich an dieser Stelle eventuell ein erster Zusammenhang zwischen Flüggellänge und Körpergewicht vermuten. Durch eine größere Flügellänge könnte man vielleicht auf einen größeren Körperbau und damit auch auf einen größeres Körpergewicht schließen. Eine gute Fragestellung für eine Regressionsanalyse. \n",
    "\n",
    "Wir vermuten erstmal einen linearen Zusammenhang zwischen Flüggellänge und Körpergewicht. Wie wir in der letzten Sitzung bereits kennengelernt haben, werden Datenanalyse-Modelle in R in der Regel durch eine Formel definiert, die eine Tilde `~` enthält. Die Tilde heißt soviel wie \"wird vorhergesagt durch\". Der Zusammenhang, den wir jetzt untersuchen, können wir also mit `flipper_length_mm ~ body_mass_g` definieren. Wir vermuten, dass das Körpergewicht linear von der Flügellänge abhängt. In `base` können wir lineare Modell mithilfe der Funktion `lm()` (für *linear model*) fitten. Wir übergeben `lm()` als erstes Argument unsere Formel `body_mass_g ~ flipper_length_mm` und an zweiter Stelle den Datensatz `penguins`. Anschließend lassen wir uns die Ergebnisse des Modells mithilfe von `summary()` ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2c448-2e9e-484b-b477-0acae96cd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(body_mass_g ~ flipper_length_mm, data = penguins) %>% \n",
    "    summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639652d-77e5-4c0a-8162-145b19bf786b",
   "metadata": {},
   "source": [
    "Hui, da kommen erstmal viele Informationen auf einmal! Für uns sind zunächst einmal zwei Teile des Outputs interessant. Unter **Coefficients** können wir uns die Koeffizienten unseres Regressionsmodells ausgeben lassen. Unser Modell folgt der Formel $ \\hat{y}_i = b_0 + b_1 * x_i $, wobei $\\hat{y}$ die Vorhersage unserer Zielgröße `body_mass_g` und $x$ unsere Prädiktorvariable `flipper_length_mm` ist. Die anderen Parameter der Regressionsgleichung $b_0$ und $b_1$ unseres Modells werden in der Zusammenfassung unter Coefficients -> Estimate ausgegeben. Der Intercept ist $b_0$ (= vorhergesagter Wert für x = 0). Die Steigung der Regressionsgerade, also $b_1$ finden wird auch unter Estimates, in der Zeile neben der unabhängigen Variable `flipper_length_mm`. In dieser Form ist das ganze für uns natürlich eher schwierig interpretierbar, daher werden wir gleich kennenlernen, wie wir das lineare Modell visualisieren können.\n",
    "\n",
    "Die zweite für uns interessante Variable ist **(Multiple) R-squared**. $R^2$, oder auch R-squared, drückt aus, wie viel der Varianz in der abhängigen Variable (hier `body_mass_g`) statistisch gesehen durch die Varianz in der unabhängigen erklärt wird (hier also `flipper_length_mm`). Mithilfe von R-squared können wir die Güte des Regressionsmodells bewerten. `0.759` ist zunächst mal ein ganz gutes R-squared, sodass wir von einem gewissen Zusammenhang zwischen `body_mass_g` und `flipper_length_mm` ausgehen können!\n",
    "\n",
    "### In `tidymodels`\n",
    "\n",
    "In `tidymodels` können wir diese Werte auch anders extrahieren, und zwar mit `tidy()` und `glance()`. Daher werden wir ab dieser Stelle mit dem `tidymodels`-Ansatz weitermachen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3958f-519e-4427-8721-157e4439d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(body_mass_g ~ flipper_length_mm, data = penguins) %>% \n",
    "    tidy()\n",
    "\n",
    "lm(body_mass_g ~ flipper_length_mm, data = penguins) %>% \n",
    "    glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c995566-63ce-440a-897c-59408cb4f20b",
   "metadata": {},
   "source": [
    "### Visualisierung\n",
    "\n",
    "Der Übersichtlichkeit halber wollen wir das lineare Modell jetzt einmal visualisieren. Dazu kombinieren zwir zwei Geome. Zunächst visualisieren wir den Zusammenhang zwischen $x$ & $y$ mit einer Punktewolke und `geom_point()`. Wir plotten `flipper_length_mm` auf der x-Achse und `body_mass_g` auf der y-Achse. Für die Regressionsgerade hat `ggplot2` praktischerweise ein eigenes **geom** eingebaut, und zwar `geom_smooth()`. Diesem Geom geben wir unsere Formel mit `y ~ x` (die Variablen haben wir im Mapping gewissermaßen \"umgetauft\"). \n",
    "Der Code dafür sieht dann folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde0842-2aef-405f-b7b4-f97dbf4a77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins %>%\n",
    "    drop_na() %>% # remove NA values\n",
    "    ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n",
    "    geom_point(size = 2) +\n",
    "    geom_smooth(method = \"lm\", formula = y ~ x) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ecbe26-f7ab-40ee-9ea1-610d887fcb3d",
   "metadata": {},
   "source": [
    "Wir können hier unseren vermuteten linearen Zusammenhang ganz gut erkennen, oder? Mit steigender Flüggellänge wird auch das Körpergewicht tendenziell schwerer. In der Realität muss man allerdings aufpassen, wo man welche Zusammenhänge vermuten kann. Wir haben in unserem Datensatz ja 3 verschiedene Spezies abgebildet, und es ist nicht automatisch gegeben, dass der Zusammenhang für alle diese Spezies gleich ist. Daher ist es sinnvoll, nochmal ein lineares Modell zu visualisieren, welches eine Regression getrennt je nach Spezies erstellt. Das funktioniert mit `ggplot2` ziemlich einfach, dafür müssen wir einfach `aes()` ein zusätzliches Argument übergeben.\n",
    "\n",
    "1. Was für eine Grafik erwartet ihr, wenn die Regression für jede einzelne Spezies ausgeführt wird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f97633-4362-4a61-a9f7-eab10759f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins %>%\n",
    "    drop_na() %>% # remove NA values\n",
    "    ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n",
    "    geom_point(size = 2) +\n",
    "    geom_smooth(method = \"lm\", formula = y ~ x) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62343332-0f70-4a06-b77d-c1659f16aba1",
   "metadata": {},
   "source": [
    "Voilà, wir bekommen jetzt einen Plot ausgegeben, bei dem wir pro Spezies ein eigenes lineares Modell abgebildet bekommen. Wenn wir uns wie weiter oben auch die genauen Koeffizienten und R-squared ausgeben lassen wollen, können wir das folgendermaßen machen.\n",
    "\n",
    "2. Bevor ihr den Code ausführt, überlegt euch, was jede Zeile macht. Schlagt dafür gerne den Befehl `do()` einmal kurz nach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87176564-3796-4c13-a80f-2f3b92633253",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins %>%\n",
    "    drop_na() %>%\n",
    "    group_by(species) %>%\n",
    "    do(tidy(lm(body_mass_g ~ flipper_length_mm, data = .)))\n",
    "\n",
    "penguins %>%\n",
    "    drop_na() %>%\n",
    "    group_by(species) %>%\n",
    "    do(glance(lm(body_mass_g ~ flipper_length_mm, data = .))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64f43e-498b-4314-9ddb-df5b54b20abd",
   "metadata": {},
   "source": [
    "3. Wie würdet ihr die Werte für `r.squared` interpretieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9641f3-8547-49f8-ac3d-45abf29bb21a",
   "metadata": {},
   "source": [
    "> Natürlich können wir auch andere Zusammenhänge abbilden. Wir könnten als Formel auch `flipper_length_mm ~ body_mass_g` angeben, wenn wir ein umgekehretes Abhängikeitsverhältnis vermuten würden. Man könnte über `+` auch noch weitere Vorhersagevariablen in das Modell einbeziehen. Wie genau man ein Regressionsmodell spezifiziert ist vor allem eine Frage von theoretischen oder sachlogischen Überlegungen. Weitere Informationen zu den Möglichkeiten zur Umsetzung in R findet ihr wie gewohnt in der Dokumentation von `lm()`.\n",
    "\n",
    "Wir werden in dieser Übung zunächst nicht weiter auf `lm()` eingehen, sondern möchten jetzt die Decision Trees von letzter Woche mit der Regressionsanalyse verbinden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60936c99-6dcd-4e07-8413-b12928a3bbc5",
   "metadata": {},
   "source": [
    "# Decision Tree Regression\n",
    "\n",
    "Eine Regression mit Decision Trees funktioniert ähnlich wie eine Klassifikation, zielt aber darauf ab, statt diskreten Kategorien kontinuierliche Werte zu prognostizieren. Wir möchten wieder die Beziehung zwischen Prädiktor- und Zielvariable ermitteln. Der Decision Tree teilt den Datensatz entlang der Prädiktorvariablen in immer homgonere Teilgruppen auf, um möglichst genau die Beziehung zur Zielvariable zu bestimmen. Nachdem das Modell trainiert wurde, können wir es wieder zur Vorhersage neuer Datenpunkte verwenden! Decision Trees für die Regressionsanalyse haben den Vorteil, auch nichtlineare Beziehungen zwischen Prädiktoren und Zielvariable abbilden zu können! \n",
    "\n",
    "Wie letzte Woche werden wir zunächst ein Beispiel gemeinsam kennenlernen. Anschließend könnt ihr das erlernte Wissen nochmal auf ein weitere Beispiel anwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580aab6-360a-4a5e-b8b6-3b0bfb88c443",
   "metadata": {},
   "source": [
    "## Bike sharing data\n",
    "\n",
    "Als erstes Beispiel schauen wir uns einen Datensatz von Hadi Fanaee-T aus dem [UCI Machine Learning Repository](http://archive.ics.uci.edu/) zum Thema Bike Sharing an.\n",
    "\n",
    "> This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.\n",
    "\n",
    "### Daten explorieren und vorbereiten\n",
    "\n",
    "Der Datensatz liegt im Ordner `dw1/data/bike_sharing/`. Dort liegt auch eine Readme-Datei, in der die Dokumentation des Datensatzes zu finden ist.\n",
    "\n",
    "4. Schaut euch die Readme-Datei des Datensatzes an und überlegt, welche Variablen von Interesse für eine genauere Untersuchung sein könnten.\n",
    "\n",
    "Anschließend laden wir den Datensatz wie gewohnt. Uns interessieren zunächst einmal die stündlichen Aufnahmen, daher laden wir die Datei `hour.csv`. Die Variable `instant` ist der Index, daher schmeißen wir diesen raus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311838c-c764-4d1d-8099-b654ba4267a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = read_csv(\"data/bike_sharing/hour.csv\", show_col_types = FALSE) %>%\n",
    "    select(-instant) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb77aa5-6275-4189-a991-40788a350509",
   "metadata": {},
   "source": [
    "Für eine Organisation, die Bike Sharing betreibt, ist vermutlich vor allem die Variable `cnt` interessant.\n",
    "\n",
    "> cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "Damit könnte eine Organisation beispielsweise eine Vorhersage darüber treffen, wieviele Fahrräder zu einem bestimmten Zeitpunkt gerade ausgeliehen sind. So könnte man beispielsweise besser planen zu welchen Zeiten besonders viele Fahrräder zur Verfügung stehen müssen. `cnt` ist in diesem Beispiel also unsere **Zielvariable**! Bevor wir ans Modellieren gehen, wollen wir aber wie letzte Woche die Variablen nochmals mithilfe von `facet_wrap()` darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f6c63-4420-4da7-9a71-d5169c4e086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes %>%\n",
    "    select(-dteday) %>%\n",
    "    select(where(is.double)) %>%\n",
    "    pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"value\") %>%\n",
    "    ggplot(aes(x = value)) +\n",
    "        geom_histogram(bins = 30, color = \"black\", fill = \"lightblue\") +\n",
    "        facet_wrap(~variable, scales = \"free\", ncol = 4) +\n",
    "        labs(title = \"Histograms of Numeric Variables in the bike sharing data set\", x = \"Value\", y = \"Frequency\") +\n",
    "        theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b9c9e-f59a-45e5-945d-1c8fd23ccb73",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "<img src=\"https://rsample.tidymodels.org/logo.png\" alt=\"rsample\" width=\"100\" align=\"right\" /> In einem letzten Schritt der Vorbereitung teilen wir wie letzte Woche unseren Datensatz wieder in Trainings- und Testdaten auf. Die Variable `dteday` ist vom Typ `date` und daher können wir diese nicht in der Regressionsanalyse berücksichtigen (die Modelle funktionieren nur für numerische Prädiktoren). Abgesehen davon ist das Datum ja ebenfalls in den Variablen `yr` und `mnth` berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a079a9-3d34-4592-b8b1-23b8b67d9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_split <- bikes %>%\n",
    "    select(-dteday) %>%\n",
    "    select(-casual, -registered) %>%\n",
    "    initial_split(prop = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5666b-1ebc-42b2-80b7-3d75e90b00ce",
   "metadata": {},
   "source": [
    "### Modell trainieren\n",
    "\n",
    "<img src=\"https://recipes.tidymodels.org/logo.png\" alt=\"recipe\" width=\"100\" align=\"right\" /> Für das Modellieren gehen wir recht analog vor wie bei der Klassifikation mit Decision Trees. Allerdings wollen wir diese Woche das ganze noch etwas formalisieren. Im `tidymodels` Universum gibt es ein Paket, mit dem wir den Trainings- und Testprozess noch etwas Form geben können, und zwar das Paket `recipe`. Mit `recipe` können wir analog zu `dplyr` Pipe-Sequenzen von bestimmt Features des Modellierens erstellen. Dadurch können wir den ganzen Prozess etwas formalisieren und verallgemeinern. Außerdem beinhaltet `recipe` einige praktische Funktionen, die das Modell auch direkt noch verbessern.\n",
    "\n",
    "Wir können für unser Beispiel direkt ein einfaches Rezept erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477acbbf-a6fb-440a-80fc-27b366180863",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_recipe <- training(bikes_split) %>%\n",
    "    recipe(cnt ~ .) %>%\n",
    "    prep() %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60073d77-4428-41fa-8b9b-fa5a1b045e85",
   "metadata": {},
   "source": [
    "Auch hier benutzen wir wieder die Tilde, um die Beziehung unserer Zielvariablen zu definieren. `cnt ~ .` bedeutet hier einfach, dass wir `cnt` als Zielvariable (outcome) und alle anderen Variablen als Prädiktoren (predictor) definieren! Wir setzen an dieser Stelle tatsächlich noch gar nicht fest, was wir eigentlich für ein Modell verwenden wollen (also ob z.B. lineare Regression oder ein Decision Tree). Außerdem gibt uns `recipe` auch direkt Informationen über den Umfang der Trainingsdaten.\n",
    "\n",
    "In der Regel gibt es bei einem Datensatz mit so vielen Variablen auch Korrelationen unter den Prädiktoren. Das liegt gerade bei Wetterdaten auf der Hand - die `season` (Jahreszeit) hat z.B. vermutlich einen Einfluss auf die Temperatur `temp`. Stark korrelierende Prädiktoren stellen ein Problem für die Analyse dar. Man kann sie ausschließen, um die Genauigkeit des Modells zu verbessern. Dafür kennt `recipe` praktischerweise die Funktion `step_corr()`, welche wir mithilfe der Pipe einfach als zusätzlichen Schritt ins Rezept einbauen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd6b92-79b0-4adc-a722-f7e450bcc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_recipe <- training(bikes_split) %>%\n",
    "    recipe(cnt ~ .) %>%\n",
    "    step_corr(all_predictors()) %>%\n",
    "    prep() %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b04112-f3b1-4558-9624-0261da6b4184",
   "metadata": {},
   "source": [
    "Unser Rezept enthält jetzt auch noch den Abschnitt Operations. Wie wir sehen können filtert `recipe` automatisch die gefühlte Temperatur in Celsius `atemp` raus. Diese korreliert vermutlich mit der gemessenen Temperatur in Celsius `temp`, und zwar so stark, dass es besser ist,  sie für die Analyse ausschließen. \n",
    "\n",
    "Zwei weitere Operationen die sich bei kontinuierlichen numerischen Daten lohnen können, sind **Centering** und **Normalisation**, in Kombination auch **Standardisation** (oder z-standardisation) genannt. Dabei werden die Datenpunkte zentriert, sodass der Mittelwert jedes Prädiktors 0 ist, und anschließend normalisiert (genormt), sodass jeder Prädiktor eine Standardabweichung von 1 hat. Die Prädiktor-Variablen werden so auf einen ähnlichen Wertebereich gebracht. Das ist einerseits praktisch, wenn man ihre Einflüsse untereinander vergleichen will. Ist man redoch daran interessiert zu interpretieren, um wie viel man einen Prädiktor erhöhen oder verringern muss, um die abhängige Varable um eine Einheit zu verändern, sind nicht-standardisierte Werte geeigneter.\n",
    "\n",
    "Wir entscheiden uns für Standardisierung und setzen das Ganze mit `recipe` wie folgt um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a834f-f799-4470-b263-513813bac9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_recipe <- training(bikes_split) %>%\n",
    "    recipe(cnt ~.) %>%\n",
    "    step_corr(all_predictors()) %>% # remove correlating predictors\n",
    "    step_center(all_predictors(), -all_outcomes()) %>% # centering\n",
    "    step_scale(all_predictors(), -all_outcomes()) %>% # normalisation\n",
    "    prep() %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e47e9d-00d5-4627-9149-055bd6f624b8",
   "metadata": {},
   "source": [
    "Fertig! Wir haben jetzt unser Rezept für das Modell definiert! Das Tolle an `recipe` ist, dass wir dafür den eigentlichen Datensatz `bikes` gar nicht verändern mussten! `recipe` merkt sich einfach, welche Operationen wir auf den Datensatz anwenden wollen und führt das ganze dann nur aus, wenn wir es zum Trainieren oder Testen benötigen. Außerdem könnten wir mit diesem Datensatz jetzt auch verschiedene Modelle trainieren! Wir haben also eine Art Kochrezept für die richtige Vorbereitung der Daten geschrieben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd982bf7-4f19-457e-87fc-0758e1fb7aed",
   "metadata": {},
   "source": [
    "#### `bake()` and `juice()`\n",
    "\n",
    "Wenn wir die Trainings- und Testdaten jetzt als eigenen Datensatz festlegen wollen, müssen wir das Rezept anwenden! Um Trainingsdaten zu erstellen, benutzen wir die Funktion `juice()`. Diese nimmt sich das Rezept und wendet es auf die Trainingsdaten an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fdfc8-ea77-4677-954b-bdd4738babba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_training <- bikes_recipe %>%\n",
    "    juice() %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f9be7-e46e-4c61-bc09-b883e34b8693",
   "metadata": {},
   "source": [
    "Wir können sehen dass alle Prädiktoren auf einen Bereich um 0 herum normalisiert worden sind. Das macht es für jetzt umso schwieriger zu lesen, da wir nicht mehr genau sagen können, was die Daten eigentlich bedeuten. Für unser Modell wird es jetzt aber einfacher, gute Vorhersagen zu treffen!\n",
    "\n",
    "Analog können wir mithilfe von `bake()` die Testdaten ebenfalls vorbereiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1958d8-0870-4737-b136-0b4536d14607",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_testing <- bikes_recipe %>%\n",
    "    bake(testing(bikes_split)) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32864922-9afe-4c8f-a71d-125ea30fd854",
   "metadata": {},
   "source": [
    "#### Modellauswahl\n",
    "\n",
    "Jetzt kommen wir endlich zum eigentlichen Modellieren! Wie letzte Woche können wir uns wieder ein Modell zusammenbasteln. Wir benutzen dafür wieder `decision_tree()` mit der Engine `rpart`, wollen diesmal den Modus aber auf `regression` statt auf `classification` setzen. Als Trainingsdaten benutzen wir die zuvor standardisierten Daten `bikes_training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ef7cf-515f-4911-9796-000fb995127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_model <- decision_tree() %>%\n",
    "    set_engine(\"rpart\") %>%\n",
    "    set_mode(\"regression\") %>%\n",
    "    fit(cnt ~ ., data = bikes_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d060543-3da4-496e-b2d1-c1795d65e836",
   "metadata": {},
   "source": [
    "Mithilfe von `rpart.rules()` können wir uns die Regeln unseres trainierten Decision Trees ausgeben lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a17c21-cf77-43eb-ae68-d9f90057a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_model$fit %>%\n",
    "    rpart.rules(roundint = FALSE) %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd0bea-6a91-4479-913b-6823d160a50b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #efe3fd\"><h1>Präsenzteil</h1></div> \n",
    "\n",
    "## Bike sharing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c6477-8ad2-4d9d-9541-31262d4b2963",
   "metadata": {},
   "source": [
    "### Modell evaluieren\n",
    "\n",
    "<img src=\"https://parsnip.tidymodels.org/logo.png\" alt=\"parsnip\" width=\"100\" align=\"right\"/> Wie zuvor können wir die Funktion `predict()` aus dem Paket `parsnip` verwenden, um Ergebnisse für den Testdatensatz vorherzusagen. Mithilfe von `bind_cols()` verbinden wir dann die Testdaten mit den vorhergesagten Werten.\n",
    "\n",
    "<img src=\"https://yardstick.tidymodels.org/logo.png\" alt=\"parsnip\" width=\"100\" align=\"right\"/> Die Evaluation erfolgt dann wieder mithilfe des Pakets `yardstick`. Da wir diesmal eine Regressionsanalyse vorgenommen haben statt einem Klassifizierungsproblem, brauchen wir andere Metriken! Wie schon weiter oben können wir wieder einen Satz an Metriken definieren, diesmal interessiert uns aber vor allem R-squared und nicht mehr die Genauigkeit des Modells.\n",
    "\n",
    "5. Was bedeuten die Metriken `rmse` und `mae`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb077744-12a7-4d6c-ab31-9a7513748e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics <- metric_set(rsq, rmse, mae)\n",
    "\n",
    "bikes_model %>%\n",
    "    predict(bikes_testing) %>%\n",
    "    bind_cols(bikes_testing) %>%\n",
    "    metrics(truth = cnt, estimate = .pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d8aff-1327-4586-8210-352ec3fc8ede",
   "metadata": {},
   "source": [
    "Unser Modell erreicht ein R-squared von etwa 0.7, was gar nicht so schlecht ist! Es Bedeutet, dass 70 Prozent der Varianz in der abhängigen Variablen durch die unabhängigen erklärt werden kann. Das lässt uns hoffen, dass man auf basis der beobachteten Prädiktorvariablen eine gute Vorhersage treffen kann, wie viele Bikes ausgeliehen sein werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9484108-d5b8-47c2-b149-4bd0164e754f",
   "metadata": {},
   "source": [
    "#### Variable importance\n",
    "\n",
    "Um den Einfluss der einzelnen Prädiktoren abzuschätzen, können wir wieder die **Variable Importance** berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654de30-9ff4-4bad-be4f-f9cfa441bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_model %>% vi(scale = TRUE) %>% # scale the most important variable to 100\n",
    "    head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bcac8-bfb2-43dc-9a2f-9d5b527cd821",
   "metadata": {},
   "source": [
    "Den größten Einfluss hat die Variable `hr`, welche für die Tagesuhrzeit steht. Auch ziemlich entscheidend ist die Temperatur `temp`. Das lässt sich beides relativ klar begründen – nachts fahren vermutlich weniger Menschen mit Bike Sharing als tagsüber, ebenso fahren mehr Menschen wenn es warm genug ist, und weniger Menschen, wenn es draußen kalt ist. In einem richtigen Anwendungsfall ist es sehr wichtig zu verstehen, welche Prädiktoren welchen Einfluss auf die Zielvariable haben.\n",
    "\n",
    "Um das besser nachzuvollziehen, kann man auch an dieser Stelle nochmal versuchen, die einzelnen Variablen zu plotten. Z.B. können wir den durchschnittlichen Count an ausgeliehen Fahrrädern `cnt` pro Stunde darstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe1149-6238-4436-901f-bc1daa40dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes %>%\n",
    "    select(hr, cnt) %>%\n",
    "    group_by(hr) %>%\n",
    "    summarise(cnt = mean(cnt)) %>%\n",
    "    ggplot(aes(x = hr, y = cnt)) +\n",
    "    geom_line() +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d5d4c-d7e8-4f32-b532-ffe493d9469a",
   "metadata": {},
   "source": [
    "Hier sehen wir bereits, dass die Tageszeit einen starken Einfluss auf `cnt` hat. Nachts gibt es einen Zeitraum, an dem fast gar keine Fahrräder ausgeliehen sind, außerdem gibt es zwei Peaks zu den Stoßzeiten 8 und 17 Uhr, was vermutlich auf Fahrten zwischen Wohnung und Arbeitsstelle zurückzuführen ist.\n",
    "\n",
    "Auch für die Temperatur können wir die Abhängigkeit von `cnt` zur Temperatur `temp` darstellen und dadurch die Abhängigkeit der Zielvariable `cnt` zum Prädiktor `temp` besser nachvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cc17c-0109-423e-8730-e68b0c345271",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes %>%\n",
    "    select(temp, cnt) %>%\n",
    "    group_by(temp) %>%\n",
    "    summarise(cnt = mean(cnt)) %>%\n",
    "    ggplot(aes(x = temp, y = cnt)) +\n",
    "    geom_line() +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c013e-dfef-4bcc-8b56-aa9630a3cfed",
   "metadata": {},
   "source": [
    "### Vorhersagen treffen\n",
    "\n",
    "Wie bei den Pilzen müssen wir jetzt einen neuen `tibble` erstellen, den wir anschließend mithilfe von `predict()` vorhersagen können. Dabei ist zu beachten, dass einige der Prädiktorvariablen vorher bereits im Datensatz normalisiert worden waren! Das ist im Readme hinterlegt und im folgenden Code auch nochmal kommentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d50ce-3951-475e-b445-465b389eb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "vorhersage <- tibble(\n",
    "    season = 4, # 1 spring, 2 summer, 3 fall, 4 winter\n",
    "    yr = 0, # 0 for 2011, 1 for 2012\n",
    "    mnth = 2, # 2 for February\n",
    "    hr = 11, # 11:00\n",
    "    holiday = 0, # 0 no holiday, 1 is holiday\n",
    "    weekday = 5, # weekday\n",
    "    workingday = 1, # 0 no working day, 1 week day\n",
    "    weathersit = 2, \n",
    "    # weathersit\n",
    "    # 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    # 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    # 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    # 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "    \n",
    "    temp = 30 / 41, # Celsius, normalised temperature through division by 41\n",
    "    atemp = 25 / 50, # Celsius, normalised feeling temperature through division by 50\n",
    "    hum = 20 / 100, # percentage, normalised through division by 100\n",
    "    windspeed = 35 / 67, # normalised through division 67\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd2be4-aa88-43e8-95eb-b460fb846818",
   "metadata": {},
   "source": [
    "Anschließend müssen wir die Daten wieder standardisieren, damit sie zu den Trainingsdaten passen! Das machen wir wieder mithilfe von `bake()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411f1d5-b730-49bc-ba70-4efc6cd0d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vorhersage_standardised <- bikes_recipe %>%\n",
    "    bake(vorhersage) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4022b2d-aa85-430a-8609-2f1dbe20b0eb",
   "metadata": {},
   "source": [
    "Nun können wir die Vorhersage mit `predict()` treffen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9ff26-70e8-4dd4-b529-9b0ca250831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_model %>%\n",
    "    predict(vorhersage_standardised) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86763032-74aa-44f4-9581-8a7c4965986d",
   "metadata": {},
   "source": [
    "Wir könnten jetzt also für einen beliebigen Zeitpunkt die Anzahl der ausgeliehen Fahrräder vorhersagen, sofern uns die notwendigen Wetterdaten zur Verfügung stehen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9dad5-6871-4d3d-a5fc-a051159a0d37",
   "metadata": {},
   "source": [
    "### Modell verbessern\n",
    "\n",
    "Für die Modellverbesserung gibt es neben den bisher kennengelernten Kniffen wie einer größeren Datenbasis auch die Möglichkeit, ein verbessertes Modell zu wählen. Die Modelle, die wir bisher verwendet haben, sind für erstmal ziemliche Blackboxen, sofern wir nicht direkt den Quellcode einsehen oder uns tief in die Dokumentation einlesen. Daher bleibt uns am ehesten übrig, nach weiteren R-Paketen zu suchen, die eine verbesserte Regressionsanalyse mit Decision Trees umsetzen. Ein Beispiel hierfür ist das Paket [Cubist](https://cran.r-project.org/web/packages/Cubist/index.html). Dieses basiert auf Quinlan's M5 model und ist ein typisches Beispiel, bei dem eine Funktion aufbauend auf einem Paper umgesetzt wurde. \n",
    "\n",
    "> Quinlan, J. R. (1992, November). Learning with continuous classes. In 5th Australian joint conference on artificial intelligence (Vol. 92, pp. 343-348).\n",
    "\n",
    "Um den Algorithmus besser zu verstehen, wäre dann der nächste Schritt, das Paper zu lesen. Dafür haben wir natürlich gerade keine Zeit. Tendenziell ist es aber wünschenswert, dass man ein Grundverständnis von den Verfahren hat, die man anwendet. Spannend ist auch noch das Jahr, in dem das Paper veröffentlich wurde (1992). Das ist schon über 30 Jahre her, und war damals state-of-the-art Machine Learning. Bis ChatGPT ist es also noch ein weiter Weg..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c12d5-7a1e-4fab-8ca5-ba5f878895a1",
   "metadata": {},
   "source": [
    "## Zweites Anwendungsbeispiel\n",
    "\n",
    "Und nun zu einem komplett neuen Thema: Es geht jetzt um \"Abalone\", auch Seeohren genannt. Das ist eine Meeresschnecken-Art. Mehr zu diesen faszinierenden Tieren findet ihr auf [Wikipedia](https://de.wikipedia.org/wiki/Seeohren). Die Aufgabe, der wir uns nun stellen, ist die Bestimmung des Alter einzelner Exemplare der Gattung. Hier ein paar Infos zum Forschungszusammenhang und Datensatz:\n",
    "\n",
    "> Predicting the age of abalone from physical measurements.  The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task.  Other measurements, which are easier to obtain, are used to predict the age.  Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\n",
    ">\n",
    "> From the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).\n",
    "\n",
    "[Abalone data](http://archive.ics.uci.edu/dataset/1/abalone)\n",
    "\n",
    "> Nash,W., Sellers,T., Talbot,S., Cawthorn,A., and Ford,W. (1995). Abalone. UCI Machine Learning Repository. https://doi.org/10.24432/C55C7W.\n",
    "\n",
    "6. Schaut im Readme unter `\"data/abalone/abalone.names\"` die Bedeutung der verschiedenen Variablen des Datensatzes nach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7fe67-5c74-411d-948c-b8862bf33fd9",
   "metadata": {},
   "source": [
    "7. Ladet den Abalone-Datensatz aus dem Pfad `\"data/abalone/abalone.data\"` in einen neuen Dataframe und exploriert die verschiedenen Variablen des Datensatzes.  In Bezug auf das Zitat weiter oben aus der ursprünglichen Veröffentlichung des Datensatzes - welche Variable stellt eine interessante Zielvariable dar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885e277-1cab-4925-9952-79d0fbc61b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06a0808a-f4ea-4b96-993f-eb7db214a64e",
   "metadata": {},
   "source": [
    "8. Stellt die verschiedenen Variablen mithilfe von `ggplot2` und `facet_wrap()` dar. Wie sind die Variablen verteilt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e17f59-ac22-4e58-94f2-e30d95305377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537839a7-c41b-4f53-b363-926d3a377df3",
   "metadata": {},
   "source": [
    "9. Teilt den Datensatz mithilfe von `initial_split()` in Trainings- und Testdaten auf. Achtung: ihr dürft lediglich numerische Variablen verwenden – eventuell müsst ihr einige Variablen vorher herausfiltern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0b0e1-3c37-4ed5-94bf-fe19e6094ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0470e43-38f6-4f3c-b40d-c5b518717bee",
   "metadata": {},
   "source": [
    "10. Erstellt mithilfe von `recipe()` ein Rezept für die Trainingsdaten. Findet mithilfe von `step_corr()` heraus, welche Variablen miteinander korrelieren. Diskutiert, warum eine Korrelation zwischen den Variablen naheliegend ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e460f-f6d8-4bf5-b7b4-63caee4e4121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4bae23e-11e6-4684-b397-7ccce740185d",
   "metadata": {},
   "source": [
    "11. Im nächsten Schritt müsst ihr `bake()` und `juice()` verwenden und das vorher erstelle `recipe` auf die Trainings- und Testdaten anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1dde93-1c68-4b26-8994-13685a0888cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81460de4-6aec-464b-9f38-2710f7f1c81d",
   "metadata": {},
   "source": [
    "12. Erstellt ein Modell vom Typ `decision_tree()` mit der Engine `\"rpart\"` und dem Modus `\"regression\"` und trainiert das Modell mit den Trainingsdaten `abalone_training`. Schaut euch anscheinend die Regeln des Modells an und überlegt kurz, welche Prädiktoren wohl eine große Rolle spielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf90995-2a69-4997-822c-38320447a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cedb0f6-5ebb-4d49-9207-8e719f15f252",
   "metadata": {},
   "source": [
    "13. Berechnet mithilfe der Testdaten `abalone_testing` sowie `predict()` und `metrics()` den R-squared value des Modells. Fragt anschließend ChatGPT oder Google, wie ihr das Ergebnis interpretieren würdet. Ist das Ergebnis gut genug für den Anwendungsfall? Inwiefern hängt die Beantwortung dieser Frage vom Kontext der Fragestellung ab (also der Fachdisziplin)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f394b-ac7c-4993-9678-fd37cd673b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1fbd7d4-d195-4c20-8ef7-66183c92d171",
   "metadata": {},
   "source": [
    "14. Was für ein Alter (Anzahl der Ringe + 1.5) würdet ihr für eine Abalone mit `height = 0.85`, `weight_shucked = 0.6` und `weight_shell = 0.8` vorhersagen? Vergesst nicht, die neuen Daten vorher mit `bake()` zu standardisieren. Was ist der Mean Absolute Error `mae` für das Ergebnis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff794c2a-dd53-4e04-a1de-97a8cb2658fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568facef-4896-4a5e-bed1-739beff4a34a",
   "metadata": {},
   "source": [
    "## Guess the Correlation\n",
    "\n",
    "Als Abschluss der Übung wollen wir noch ein kleines Spiel zusammen spielen, und zwar [GeoGebra Guess the Correlation](https://www.geogebra.org/m/gvanchqz). Klickt auf die Webseite des Spiels und fangt an, das Spiel zusammen zu spielen. Das Spiel ist sehr simpel und relativ selbsterklärend, es geht darum, den R-squared Wert einer Verteilung zu erraten. Dadurch bekommt ihr ein Gefühl für die Güte von linearen Modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bc524-cced-4b83-a6ba-77c0b69fbb54",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In dieser Übung haben wir die Regressionsanalyse kennengelernt. Regressionen sind eines der am häufigsten verwendeten Tools in der Statistik. Sie werden euch immer wieder begegnen. Die Umsetzung in R kann sowohl mit `base`, innerhalb von Grafiken mit `ggplot2` als auch mit `tidymodels` erfolgen, und wir können die Regressionsanalyse auch mit weiteren ML-Modellen wie z.B. Decision Trees verknüpfen.\n",
    "\n",
    "Außerdem haben wir in dieser Übung den Modellierungsprozess etwas formalisiert, und zwar mithilfe des Pakets `recipe`. Dieses Paket hilft uns dabei, stringente und gleichartige Abläufe für unsere Modelle zu bauen.\n",
    "\n",
    "In der nächsten Sitzung wird es um den **Naive Bayes Classifier** gehen. Dabei werden wir nochmal auf `recipe` verzichten, und einige Funktionen des Modells selber schreiben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b44186-e856-4d73-b1eb-ae55131dd1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
