{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746c8be6-2820-4f85-9602-625837e9c222",
   "metadata": {
    "id": "746c8be6-2820-4f85-9602-625837e9c222"
   },
   "source": [
    "# LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261f403-af76-4c44-b915-92c02d4bd4fe",
   "metadata": {
    "id": "1261f403-af76-4c44-b915-92c02d4bd4fe"
   },
   "source": [
    "Für den Themenkomplex Deep Learning in R ist folgendes Buch zu empfehlen:\n",
    "\n",
    "> Chollet, F., Kalinowski, T. & Allaire, J. J.  (2022). Deep Learning with R, (2nd Edition). Manning.\n",
    "\n",
    "Bevor wir beginnen sei nochmal kurz gesagt, dass das hier das letzte Notebook ist. Es enthält einige neue Aspekte des Deep Learnings und könnte an einigen Stellen etwas kompliziert sein. Daher versucht immer im Kopf zu behalten, dass ihr nicht alle Schritte komplett verstehen müsst. Es gibt ganze Studiengänge, die sich nur mit dem Thema Deep Learning beschäftigen, daher können wir in zwei Sitzungen nur einen ganz groben Überblick für das Thema geben. Für reale Anwendungen würden wir vermutlich auch ohnehin vortrainierte und wesentlich komplexere Modelle verwenden. Es ist aber bestimmt trotzdem hilfreich, 1-2 weitere Anwendungen von neuronalen Netzwerken kennenzulernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a85a4-db21-48bc-a654-96fbb6a5aeac",
   "metadata": {
    "id": "b66a85a4-db21-48bc-a654-96fbb6a5aeac"
   },
   "source": [
    "## LSTM Classification\n",
    "\n",
    "**Long short-term memory** (LSTM) networks bezeichnet eine bestimmte Art von neuronalen Netzwerken. Diese sind analog zu einer Art Kurzzeitgedächtnis in der Lage, Informationen über einen kurzen Zeitraum zu speichern. Die Idee der LSTM-Netze stammt aus den späten 1990er Jahren und ermöglicht besonders seit 2016 in Kombination mit der verbesserten Performance von Microchips und den dadurch deutlich größeren möglichen Trainingsdatenmengen große Erfolge im Bereich des maschinellen Lernens.\n",
    "\n",
    "In ihrer einfachsten Form verfügt jede LSTM-Unit über 4 interne Parameter (im Gegensatz zu 1 Parameter in der einfachen Art der neuronalen Netze, welche wir letzte Woche kennengelernt haben).\n",
    "* Das *Input-Gate* bestimmt, in welchem Maß ein neuer Wert in die Zelle fließt.\n",
    "* Das *Forget-Gate* bestimmt, in welchem Maß (wie schnell) die Zelle den Wert wieder vergisst.\n",
    "* Das *Output-Gate* bestimmt, in welchem Maß der Wert der Zelle an den nächsten Layer übergeben wird.\n",
    "\n",
    "Leider können wir hier aufgrund des zeitlichen Umfangs dieser Übung nicht besonders detailliert auf die Funktionsweise von LSTM-Netzen eingehen. Wichtig ist, dass sie durch ihre Fähigkeit, **Informationen kurzfristig zu speichern**, ein breites Feld an Anwendungen ermöglicht haben und ihre Verwendung in den letzten zehn Jahren stark zugenommen hat.\n",
    "\n",
    "In dieser Übung werden wir zunächst ein einfaches LSTM-Netzwerk selber trainieren, um zu klassifizieren wie Tweets über Fluggesellschaften sich je nach der Stimmung, die darin geäußert wird unterschieden. Diese Aufgabe ähnelt stark den **Klassifizierungsproblemen**, die wir bereits kennengelernt haben. Diesmal benutzen wir statt eines \"klassischen\" ML-Algorithmus eben ein neuronales Netzwerk.\n",
    "\n",
    "Im zweiten Teil der Übung werden wir nochmal kurz auf die **Textgenerierung** mithilfe von LSTM-Netzwerken eingehen und auf der Datengrundlage von Büchern von Jane Austen und Songtexten von Taylor Swift einige zusammenhängende englische Phrasen generieren.\n",
    "\n",
    "Als erstes laden wir die notwendigen Pakete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23aebd-fe8b-452c-841e-cd0c2bc304d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de23aebd-fe8b-452c-841e-cd0c2bc304d4",
    "outputId": "42056163-8bae-4256-cbc4-781a30176bd8"
   },
   "outputs": [],
   "source": [
    "pacman::p_load(\n",
    "    httr,\n",
    "    keras,\n",
    "    tensorflow,\n",
    "    tokenizers,\n",
    "    textclean,\n",
    "    tidymodels,\n",
    "    tidyverse\n",
    "    )\n",
    "\n",
    "set.seed(321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e039388-2fdb-466d-b76f-af937b682fa8",
   "metadata": {
    "id": "8e039388-2fdb-466d-b76f-af937b682fa8"
   },
   "source": [
    "## Datengrundlage\n",
    "\n",
    "Als Datengrundlage dienen uns [Twitter-Daten](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment), die sich auf US-amerikanische Airlines beziehen. Sie stammen ursprünglich aus der [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone).\n",
    "\n",
    "Der Datensatz enthält knapp 15.000 Tweets. Jeder Tweet ist durch eine bestimmte Stimmung `airline_sentiment` charakterisiert. Die Stimmung kann entweder `negative`, `neutral` oder `positive` sein. Wir erzeugen später ein LSTM-Netzwerk, welches auf Basis der Tweets lernt, neue Tweets in einer der drei Stimmungen zu charakterisieren.\n",
    "\n",
    "Wir können den Datensatz mithilfe von `read_csv()` direkt von Github herunterladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f08b71-d5fe-40ea-830e-31ead136de12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43f08b71-d5fe-40ea-830e-31ead136de12",
    "outputId": "ee08319f-b27a-49e9-bddb-76e3f5f3ca01"
   },
   "outputs": [],
   "source": [
    "url <- \"https://raw.githubusercontent.com/ahmadhusain/lstm_text/master/data_input/tweets.csv\"\n",
    "\n",
    "data_original <- read_csv(url, show_col_types = FALSE) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f458d-dc44-4c2a-8b48-1f9d41941a2f",
   "metadata": {
    "id": "cc9f458d-dc44-4c2a-8b48-1f9d41941a2f"
   },
   "source": [
    "Wie wir sehen, enthält der Datensatz noch viele andere Informationen. Uns interessieren aber nur der Textinhalt in der Spalte `text` sowie die Stimmung `airline_sentiment`.\n",
    "\n",
    "In einer der Übungen im ersten Block hatten wir bereits mit Textbereinigungstools gearbeitet. Auch hier müssen wir wieder den Text bereinigen. Dafür nutzen wir die beiden Pakete `textclean` und `stringr`.\n",
    "\n",
    "* Mit `textclean` können wir URLs, Emojis und HTML-Tags entfernen: `replace_url()`, `replace_emoji()`, `replace_emoticon()`, `replace_html()`\n",
    "* `str_remove_all(pattern = pattern)` von `stringr` entfernt bestimmte Patterns. Als Patterns können wir sogenannte **Regular Expressions** angeben. Das sind allgemeingültige Formeln für bestimmte Zeichen. Der Pattern `\"@\\\\w+\"` entfernt z.B. alle Hashtags.\n",
    "* `replace_contraction()` und `replace_word_elongation()` aus `textclean` ersetzt Wortkürzungen oder -streckungen, wie z.B. \"c u\" (für \"see you\") oder \"heeeeey\" (für \"hey\").\n",
    "* Außerdem wollen wir alle Wörter kleinschreiben mithilfe von `str_to_lower()` und die Leerzeichen zwischen Wörtern und Tweets vereinheitlichen mithilfe von`str_squish()`.\n",
    "\n",
    "Das ganze schreiben wir wieder in einer eigenen Funktion `string_cleaner()` (so wie bereits in Übung 09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e20d3-41ab-480e-8251-0d9c4e7909ed",
   "metadata": {
    "id": "751e20d3-41ab-480e-8251-0d9c4e7909ed"
   },
   "outputs": [],
   "source": [
    "string_cleaner <- function(text_vector) {\n",
    "    text_vector %>%\n",
    "      replace_url() %>% # from textclean\n",
    "      replace_emoji() %>% # from textclean\n",
    "      replace_emoticon() %>% # from textclean\n",
    "      replace_html() %>% # from textclean\n",
    "      str_remove_all(pattern = \"@\\\\w+\") %>% # remove mentions (@), from stringr\n",
    "      str_remove_all(pattern = \"#\\\\w+\") %>% # remove hastags, from stringr\n",
    "      replace_contraction() %>% # from textclean\n",
    "      replace_word_elongation() %>% # from textclean\n",
    "      str_replace_all(pattern = \"\\\\?+\", replacement = \" questionmark \") %>% # from stringr\n",
    "      str_replace_all(pattern = \"\\\\!+\", replacement = \" exclamationmark \") %>% # from stringr\n",
    "      str_remove_all(pattern = \"[[:punct:]]\") %>% # from stringr\n",
    "      str_remove_all(pattern = \"\\\\d\") %>% # remove numbers, from stringr\n",
    "      str_remove_all(pattern = \"\\\\$\") %>% # remove dollar signs, from stringr\n",
    "      str_to_lower() %>% # from stringr\n",
    "      str_squish() # from stringr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475feda-ba3b-4602-87e1-6e2b1041ed1b",
   "metadata": {
    "id": "2475feda-ba3b-4602-87e1-6e2b1041ed1b"
   },
   "source": [
    "Anschließend müssen wir die Funktion auf jede Zeile der Spalte `text` anwenden. Das sollte maximal eine Minute dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb5719-8e03-4932-9ca2-1a31ac9eab63",
   "metadata": {
    "id": "b8eb5719-8e03-4932-9ca2-1a31ac9eab63"
   },
   "outputs": [],
   "source": [
    "data <- data_original %>%\n",
    "    mutate(text_clean = as.character(map(text, string_cleaner)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65272ae9-7d09-4928-8191-c4273f9fe133",
   "metadata": {
    "id": "65272ae9-7d09-4928-8191-c4273f9fe133"
   },
   "source": [
    "Jetzt können wir uns beispielhaft einige Tweets ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461a2c2-44fc-49ac-ace2-704ada59b9eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "0461a2c2-44fc-49ac-ace2-704ada59b9eb",
    "outputId": "df9effd8-15ec-43c0-ee40-8786e99d4d7c"
   },
   "outputs": [],
   "source": [
    "data %>%\n",
    "  select(text_clean) %>%\n",
    "  sample_n(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27179e97-a792-479e-bc6c-97a6c89ae815",
   "metadata": {
    "id": "27179e97-a792-479e-bc6c-97a6c89ae815"
   },
   "source": [
    "Super, das sieht gut aus! Wir haben zwar gerade einige Informationen verloren, aber dafür haben wir nur noch Kleinbuchstaben und Leerzeichen in unserem Vokabular. Das erleichtert das Training für das neuronale Netzwerk immens!\n",
    "\n",
    "Im nächsten Schritt schmeißen wir alle Spalten raus, die wir nicht brauchen, und wandeln die Sentiments in numerische Label um:\n",
    "\n",
    "* 0 für \"negative\"\n",
    "* 1 für \"neutral\"\n",
    "* 2 für \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef771e6-9523-4602-8c43-2c727febda9d",
   "metadata": {
    "id": "bef771e6-9523-4602-8c43-2c727febda9d"
   },
   "outputs": [],
   "source": [
    "data <- data %>%\n",
    "    mutate(\n",
    "        label = factor(airline_sentiment, levels = c(\"negative\", \"neutral\", \"positive\")),\n",
    "        label = as.numeric(label),\n",
    "        label = label - 1) %>%\n",
    "    select(text_clean, label) %>%\n",
    "    na.omit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43a8d7-349b-41a2-94c6-092ea436b0c4",
   "metadata": {
    "id": "4b43a8d7-349b-41a2-94c6-092ea436b0c4"
   },
   "source": [
    "Unser Datensatz sieht jetzt folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609d505-f471-431a-bac4-d8a1579d193d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "a609d505-f471-431a-bac4-d8a1579d193d",
    "outputId": "a7f66edc-8b75-47f6-dcf7-96e736d52f4b"
   },
   "outputs": [],
   "source": [
    "data %>% sample_n(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97948b49-b403-4574-af9b-9579d3831b6d",
   "metadata": {
    "id": "97948b49-b403-4574-af9b-9579d3831b6d"
   },
   "source": [
    "## Tokenizers\n",
    "\n",
    "Im nächsten Schritt wollen wir aus unseren Tweets sogenannte Tokens erstellen. Manchmal sind Tokens einzelne Zeichen, manchmal auch ganze Wörter.\n",
    "\n",
    "> Das Wort \"Beispiel\" könnten wir auf verschiedene Weisen tokenizen:\n",
    "> * nach einzelnen Zeichen: \"B\", \"e\", \"i\", \"s\", \"p\", \"i\", \"e\", \"l\"\n",
    "> * nach Silben: \"Bei\", \"spiel\"\n",
    "> * als einzelnes Wort: \"Beispiel\"\n",
    "\n",
    "Für unser Netzwerk nehmen wir einzelne Wörter als Token. Um die Komplexität der Trainingsdaten zu reduzieren, benutzen wir lediglich die 1024 häufigsten Wörter. Wörter, die also sehr selten vorkommen, fließen nicht in die Analyse mit ein. Das verbessert die Performance des Netzwerkes, während wir trotzdem nur vergleichsweise unwichtige Informationen verlieren.\n",
    "\n",
    "> In R gibt es verschiedene Möglichkeiten zu tokenizen, aus `tidytext`, `keras`, `tokenizers` und anderen Paketen.\n",
    "\n",
    "Wir benutzen die Funktion `text_tokenizer()` aus dem Paket `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282af9-6189-477c-a119-a0678b94f4cd",
   "metadata": {
    "id": "8b282af9-6189-477c-a119-a0678b94f4cd"
   },
   "outputs": [],
   "source": [
    "num_words <- 1024 # we only use the most frequent 1024 words for our model, the rest gets removed\n",
    "tokenizer <- text_tokenizer(num_words = num_words, lower = TRUE) %>% # sort to frequency order, then tokenize the corpus\n",
    "    fit_text_tokenizer(data$text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d81aa-87a6-4d69-9907-fe1fa8a33d92",
   "metadata": {
    "id": "5a9d81aa-87a6-4d69-9907-fe1fa8a33d92"
   },
   "source": [
    "Wie sieht unser `tokenizer` jetzt aus?\n",
    "\n",
    "Er enthält die 1024 häufigsten Wörter und gibt jedem dieser Wörter einen Index. Außerdem wird die Häufigkeit der Wörter gespeichert. Wir können das einmal beispielhaft für jeweils 5 Tokens ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1513e81-44b0-406e-a826-6ec934f820f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "d1513e81-44b0-406e-a826-6ec934f820f0",
    "outputId": "08d92508-cb41-47b7-921d-bcf6fdd930c4"
   },
   "outputs": [],
   "source": [
    "tokenizer$word_index %>% head(5)\n",
    "tokenizer$word_counts %>% head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde42ee-00f7-478c-9586-15d1fb2fb967",
   "metadata": {
    "id": "5bde42ee-00f7-478c-9586-15d1fb2fb967"
   },
   "source": [
    "## Trainings-, Validierungs- und Testdaten\n",
    "\n",
    "Im nächsten Schritt splitten wir unseren Datensatz wieder in Trainings- und Testdaten. Das kennen wir bereits aus den vorherigen Übungen!\n",
    "\n",
    "Wir zweigen allerdings von unserem Trainings-Set zusätzlich noch ein sogenanntes Validation-Set ab. Diese Validierungsdaten können während der vielen Trainingsdurchläufe vom Netzwerk für Fine-Tuning von Parametern verwendet werden. Außerdem haben wir so die Möglichkeit, das Modell frühzeitig und schon während des Trainingsprozesses zu optimieren. Die Validierungsdaten werden also im Gegensatz zu den Testdaten schon während des Trainings verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ba13a-01cb-4a0e-8988-b6f37f84ebd3",
   "metadata": {
    "id": "f29ba13a-01cb-4a0e-8988-b6f37f84ebd3"
   },
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "intrain <- data %>%\n",
    "   initial_split(prop = 0.8, strata = \"label\") # use 80% as training data\n",
    "\n",
    "data_train <- intrain %>%\n",
    "   training()\n",
    "data_test <- intrain %>%\n",
    "   testing()\n",
    "\n",
    "inval <- data_test %>%\n",
    "   initial_split(prop = 0.5, strata = \"label\") # split the test data to use 10% for testing and 10% for validation\n",
    "\n",
    "data_val <- inval %>%\n",
    "   training()\n",
    "data_test <- inval %>%\n",
    "   testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01500ac-8b55-49ad-86e8-91b74275d092",
   "metadata": {
    "id": "c01500ac-8b55-49ad-86e8-91b74275d092"
   },
   "source": [
    "Der längste vorliegende Tweet bestimmt über die Anzahl seiner Wörter die Dimension des Netzwerkes. Das Netzwerk soll so viele Inputs haben, wie der Tweet Worte lang ist. Deshalb müssen wir im nächsten Schritt herausfinden, welcher Text der längste ist bzw. wie viele Wörter er beinhaltet. Wir speichern diesen Wert im Objekt `maxlen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb7b34-13dc-4969-b0e8-26303164d5b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "93fb7b34-13dc-4969-b0e8-26303164d5b6",
    "outputId": "c2d91e1f-a0af-4f46-9f64-1f6114b89e56"
   },
   "outputs": [],
   "source": [
    "maxlen <- max(str_count(data$text_clean, \"\\\\w+\"))\n",
    "paste(\"maxiumum length words in data:\", maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99354e74-5365-481f-b33d-083c9e485e33",
   "metadata": {
    "id": "99354e74-5365-481f-b33d-083c9e485e33"
   },
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Jetzt wird es etwas komplizierter. Da das Netzwerk später mit numerischen Daten arbeiten muss, wir momentan aber nur Textdaten zur Verfügung haben, müssen wir uns einen schlauen Weg überlegen, wie der Text in numerische Vektoren umgewandelt werden kann. Eine mögliche Methode dafür ist das sogenannte [**one-hot encoding**](https://en.wikipedia.org/wiki/One-hot).\n",
    "\n",
    "Wir können one-hot encoding auf verschiedene Arten betreiben. In diesem Beispiel brauchen wir dafür den Häufigkeitsindex der einzelnen Wörter. Weiter oben hatten wir ja bereits die fünf häufigsten Wörter ausgegeben (\"to\", \"i\", \"the\", \"a\" und \"you\"). Jedes Wort erhält jetzt als Bezeichner den eigenen Häufigkeitsindex. \"to\" steht auf Platz 1, und bekommt daher den Index 1. \"i\" steht auf Platz 2 und bekommt den Index 2, usw. Damit können wir ganze Sätze in Zeichenketten übersetzen.\n",
    "\n",
    "> Beispiel: Wenn wir annehmen, das Wort \"and\" hat den Index 14, könnten wir aus der Konstruktion \"you and i\" z.B. die numerische Folge 5-14-2 erzeugen.\n",
    "\n",
    "Im Folgenden müssen wir also jeden Tweet in eine solche Zahlenkette übersetzen. Wörter, die im Tokenizer nicht vorkommen, werden einfach weggelassen. Als Resultat erhalten wir eine Matrix, die in jeder Reihe einen Tweet stehen hat, und jede Spalte steht für ein Wort. Die Dimension der Matrix ist also (Anzahl Tweets) x (längster Tweet `maxlen`). Tweets, die kürzer sind als `maxlen`, werden einfach mit Nullen aufgefüllt.\n",
    "\n",
    "Der folgende Code erledigt diese Schritte automatisch für uns. `texts_to_sequences()` übersetzt die einzelnen Wörter der Tweets in ihren Häufigkeitsindex, und `pad_sequences()` füllt die Tweets dann mit Nullen auf und erzeugt die notwendige Matrix. Außerdem übersetzen wir wie in der letzten Übung die Label in Kategorien mithilfe von `to_categorical()`. Diese Schritte wiederholen wir jeweils für die Trainings-, Validierungs- und Testdatensätze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23274542-e1e8-4baa-9db9-7fc57bab07f8",
   "metadata": {
    "id": "23274542-e1e8-4baa-9db9-7fc57bab07f8"
   },
   "outputs": [],
   "source": [
    "# prepare x vectors for the network\n",
    "data_train_x <- tokenizer %>%\n",
    "    texts_to_sequences(data_train$text_clean) %>%\n",
    "    pad_sequences(maxlen = maxlen)\n",
    "\n",
    "data_val_x <- tokenizer %>%\n",
    "    texts_to_sequences(data_val$text_clean) %>%\n",
    "    pad_sequences(maxlen = maxlen)\n",
    "\n",
    "data_test_x <- tokenizer %>%\n",
    "    texts_to_sequences(data_test$text_clean) %>%\n",
    "    pad_sequences(maxlen = maxlen)\n",
    "\n",
    "# prepare y vectors for the network\n",
    "data_train_y <- data_train$label %>% to_categorical(num_classes = 3)\n",
    "data_val_y <- data_val$label %>% to_categorical(num_classes = 3)\n",
    "data_test_y <- data_test$label %>% to_categorical(num_classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8866435c-855f-4b36-9f5c-60e4f34b14fb",
   "metadata": {
    "id": "8866435c-855f-4b36-9f5c-60e4f34b14fb"
   },
   "source": [
    "Wir können `data_train_x` einmal beispielhaft ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf26c9-b53b-45a5-8b0b-9679ab2e592b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "65cf26c9-b53b-45a5-8b0b-9679ab2e592b",
    "outputId": "e220ec31-ac69-4ad0-e27d-02b06a124da9"
   },
   "outputs": [],
   "source": [
    "data_train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dcfbe-c62e-43c7-8981-beb9ed1a3b09",
   "metadata": {
    "id": "ec1dcfbe-c62e-43c7-8981-beb9ed1a3b09"
   },
   "source": [
    "## Model definition\n",
    "\n",
    "Die Datengrundlage ist geschaffen. Diese Matrix können wir jetzt super nutzen, um unser Modell zu trainieren. Als nächstes müssen wir das Modell definieren.\n",
    "\n",
    "Wir wollen wieder ein sequenzielles Netzwerk in Keras modellieren. Dieses hat die folgenden Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194d903-7293-400b-8d22-de2d8472aac6",
   "metadata": {
    "id": "b194d903-7293-400b-8d22-de2d8472aac6"
   },
   "outputs": [],
   "source": [
    "model <- keras_model_sequential() %>%\n",
    "\n",
    "    # the embedding input layer converts positive integers (indexes) to dense vectors of fixed size\n",
    "    layer_embedding(name = \"input\", input_dim = num_words, input_length = maxlen, output_dim = 32) %>%\n",
    "\n",
    "    # the lstm layer drops 20% of the units to per training session to prevent overfitting\n",
    "    layer_lstm(name = \"lstm\", units = 256, dropout = 0.2, recurrent_dropout = 0.2) %>%\n",
    "\n",
    "    # the output layer uses softmax as activation and has 3 units for the 3 sentiments negative, neutral and positive\n",
    "    layer_dense(name = \"output\", units = 3, activation = \"softmax\") %>%\n",
    "\n",
    "    # compile the model\n",
    "    compile(optimizer = \"adam\", metrics = \"accuracy\", loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259c659-9f7f-43ba-8f25-c51fc5ad0e40",
   "metadata": {
    "id": "d259c659-9f7f-43ba-8f25-c51fc5ad0e40"
   },
   "source": [
    "Praktischerweise können wir uns Informationen über das Modell mithilfe von `summary()` ausgeben lassen:\n",
    "\n",
    "* Welche Informationen entnehmt ihr der Zusammenfassung?\n",
    "* Welche Funktionen könnten die einzelnen Layer haben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4ac12-4329-4336-b153-3c21b922d300",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaf4ac12-4329-4336-b153-3c21b922d300",
    "outputId": "84c614e8-4bec-46ec-e399-bfca4163d33c"
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c97eb-bdc9-4cec-98d6-739d6835f2a6",
   "metadata": {
    "id": "967c97eb-bdc9-4cec-98d6-739d6835f2a6"
   },
   "source": [
    "<div style=\"background-color: #efe3fd\"><h2>Präsenzteil</h2></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d889f-557a-41ed-be66-b94ae4905d89",
   "metadata": {
    "id": "eb8d889f-557a-41ed-be66-b94ae4905d89"
   },
   "source": [
    "## Modell trainieren\n",
    "\n",
    "Jetzt geht es daran, das Modell zu trainieren. Das machen wir wieder mit der Funktion `fit()` aus `keras`. Wir können den Trainingsfortschritt in der Variable `history` abspeichern, um diesen später zu visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738729a6-7565-4bbd-bd9f-e7157eb6bd4d",
   "metadata": {
    "id": "738729a6-7565-4bbd-bd9f-e7157eb6bd4d"
   },
   "outputs": [],
   "source": [
    "# should take under 10 minutes\n",
    "\n",
    "history <- model %>% fit(\n",
    "    data_train_x, data_train_y, # this is our training data\n",
    "    batch_size = 512, epochs = 15, # 512 tweets are processed per batch, and we are training 15 epochs\n",
    "    validation_data = list(data_val_x, data_val_y) # this is our validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532fdb1-380f-49cd-b662-3168a2d3d119",
   "metadata": {
    "id": "0532fdb1-380f-49cd-b662-3168a2d3d119"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Damit wir unser Modell evaluieren können, hatten wir bei der Modelldefinition als Metrik wieder die bekannte `accuracy` definiert.\n",
    "\n",
    "Wir können die `accuracy` und den `loss` des Trainings mithilfe von `plot()` visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8031f1-4c00-45f3-b8b0-4a488e434ca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "cb8031f1-4c00-45f3-b8b0-4a488e434ca7",
    "outputId": "5a6e3a1b-ee2f-486c-e3c1-6e8deb70dc6b"
   },
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d827d8-25ff-4f8b-9f19-b43820959c93",
   "metadata": {
    "id": "a9d827d8-25ff-4f8b-9f19-b43820959c93"
   },
   "source": [
    "* Was seht ihr auf der Grafik? Wie würdet ihr die verschiedenen Kurven interpretieren?\n",
    "* Was könnte eine gute Anzahl von Epochen sein, auf die ihr das Netzwerk trainieren würdet?\n",
    "\n",
    "> Benutzt gerne eine Suchmaschine oder eine Chat-KI, falls ihr die beiden Fragen nicht beantworten könnt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648d80a-a54b-4344-b4a1-fb61596fd8c4",
   "metadata": {
    "id": "9648d80a-a54b-4344-b4a1-fb61596fd8c4"
   },
   "source": [
    "Falls das Training aus Performancegründen scheitert, sagt gerne Bescheid! Dann können wir ein trainiertes Modell auch aus dem Verzeichnis laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d2c3c-1362-4baa-8800-5a0edef3494e",
   "metadata": {
    "id": "d75d2c3c-1362-4baa-8800-5a0edef3494e"
   },
   "outputs": [],
   "source": [
    "# diesen Code nur ausführen, wenn ihr wisst was ihr tut ;)\n",
    "# model_temp <- load_model_hdf5(\"data/lstm_models/tweet_classification_15epochs.h5\")\n",
    "# save_model_hdf5(model, \"data/lstm_models/tweet_classification_15epochs.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a73352-0bd7-431e-a662-bb402cdba412",
   "metadata": {
    "id": "24a73352-0bd7-431e-a662-bb402cdba412"
   },
   "source": [
    "Wir können die Accuracy auch für die Testdaten ausgeben.\n",
    "\n",
    "* Dafür lassen wir das Modell mithilfe von `predict()` den Testdatensatz vorhersagen.\n",
    "* Anschließend ermitteln wir die Output-Units, die den größten Wert erreichen mit `k_argmax()`...\n",
    "* ...und wandeln das Ergebnis mit `k_eval()` in einen Vektoren um.\n",
    "\n",
    "Diesen können wir wie gewohnt mit Truth und Estimate mit den ursprünglichen Labels `data_test_pred` vergleichen und die `accuracy` berechnen. Und wir können eine Confusion Matrix ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7a801-e52b-43b3-89ac-3e99207dd56f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "b3b7a801-e52b-43b3-89ac-3e99207dd56f",
    "outputId": "dd0a2011-2140-44e9-b443-ea87a021646c"
   },
   "outputs": [],
   "source": [
    "# predict on testing data\n",
    "data_test_pred <- model %>%\n",
    "   predict(data_test_x) %>%\n",
    "   k_argmax() %>%\n",
    "   k_eval()\n",
    "\n",
    "# accuracy vector\n",
    "accuracy_vec(\n",
    "    truth = factor(data_test$label, labels = c(\"negative\", \"neutral\", \"positive\")),\n",
    "    estimate = factor(data_test_pred, labels = c(\"negative\", \"neutral\", \"positive\"))\n",
    ")\n",
    "\n",
    "# confusion matrix\n",
    "data.frame(\n",
    "  truth = factor(data_test$label, labels = c(\"negative\", \"neutral\", \"positive\")),\n",
    "  estimate = factor(data_test_pred, labels = c(\"negative\", \"neutral\", \"positive\"))\n",
    ") %>% conf_mat(truth, estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7bf0fa-8661-4da0-852f-ab822590bdd6",
   "metadata": {
    "id": "3b7bf0fa-8661-4da0-852f-ab822590bdd6"
   },
   "source": [
    "Wir erreichen also knapp über 80% Genauigkeit bei der Vorhersage. Das ist gar nicht so schlecht, wenn wir bedenken, dass wir ein sehr einfaches Netzwerk definiert haben!\n",
    "\n",
    "## Eigene Tweets klassifizieren\n",
    "\n",
    "Als letzten Schritt können wir einmal eigene Tweets vorhersagen. Dafür müssen wir diese analog zu unserem Textdatensatz mit `text_to_sequences()` und `pad_sequences()` in das richtige Format konvertieren. Dieses können wir dann mit `predict()` vorhersagen und mit `k_argmax()` und `k_eval()` erhalten wir wieder unser Label. Das Ganze können wir in einer kleinen Funktion verpacken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0186b6-dce7-4a0e-a461-b8a5edfd8afc",
   "metadata": {
    "id": "3f0186b6-dce7-4a0e-a461-b8a5edfd8afc"
   },
   "outputs": [],
   "source": [
    "text_analysis <- function(tweet, model, tokenizer, maxlen){\n",
    "\n",
    "    # preprocess the text data to sequences\n",
    "    sequenced <- texts_to_sequences(tokenizer, tweet) %>%\n",
    "        pad_sequences(maxlen = maxlen)\n",
    "\n",
    "    # predict the sentiment and evaluate the outcome 0, 1, or 2\n",
    "    sentiment <- model %>% predict(sequenced) %>% k_argmax() %>% k_eval()\n",
    "\n",
    "    # print the sentiment depending on the outcome\n",
    "    switch(as.character(sentiment),\n",
    "           \"0\" = print(\"Negative Sentiment\"),\n",
    "           \"1\" = print(\"Neutral Sentiment\"),\n",
    "           \"2\" = print(\"Positive Sentiment\"),\n",
    "           print(\"Unknown Sentiment\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba7a56-1154-4e1d-b5e9-e9b2b7615df2",
   "metadata": {
    "id": "64ba7a56-1154-4e1d-b5e9-e9b2b7615df2"
   },
   "source": [
    "* Testet das Netzwerk mit eigenen Texten. Funktioniert es einigermaßen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ce19c-a7ab-413e-836d-db7ca6c1c26b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d3ce19c-a7ab-413e-836d-db7ca6c1c26b",
    "outputId": "af841a8a-298c-49df-d234-c6ea46109ead"
   },
   "outputs": [],
   "source": [
    "bad <- \"i hate the service it is really bad and the toilet was disgusting\"\n",
    "bad2 <- \"this airline should be forbidden it is the worst\"\n",
    "good <- \"i love this airline. the flight was really relaxed and the food was amazing!\"\n",
    "\n",
    "good %>%\n",
    "    text_analysis(model, tokenizer, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c255d-122d-420f-aa2e-c4c3e4663e0e",
   "metadata": {
    "id": "645c255d-122d-420f-aa2e-c4c3e4663e0e"
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Analog zum Naive Bayes Spam Filter haben wir hier ein neuronales Netzwerk verwendet, um Text zu klassifizieren. Wir haben ein sehr einfaches Modell mit nur einem LSTM-Layer erstellt, welches trotzdem in der Lage ist, komplexere Zeichenketten einzuordnen. In der Realität müssten wir unser Netzwerk allerdings wesentlich verbessern, da die 80% Genauigkeit in einem tatsächlichen Szenario vermutlich eher nicht zufriedenstellend wäre. Zudem könnte man auch wieder die Datengrundlage verbessern, beispielsweise über das Entfernen von Stopwords (das hatten wir beim Spam Filter auch gemacht).\n",
    "\n",
    "Wichtig ist vor allem der Schritt der Datenaufbereitung und wie der Text vektorisiert wird. Damit wir komplexere Datenformen wie Text oder Bild verarbeiten können, müssen wir i.d.R. immer eine schlaue Parametrisierung wählen und gleichzeitig darauf achten, dass die Daten nicht zu komplex werden - sonst dauert das Training ewig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41421199-e2c8-44aa-9d91-de05a7bd2ab9",
   "metadata": {
    "id": "41421199-e2c8-44aa-9d91-de05a7bd2ab9"
   },
   "source": [
    "# LSTM Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de46b4-9d12-483d-899d-6d7643626096",
   "metadata": {
    "id": "33de46b4-9d12-483d-899d-6d7643626096"
   },
   "source": [
    "Eine weitere, viel diskutierte Anwendung von LSTM-Netzwerken ist die Generierung von Text. Dabei lernt das Netzwerk i.d.R., wie einzelne Zeichen oder Wörter aneinander gereiht werden müssen, um Text zu erzeugen. Wir bauen hier natürlich nicht ChatGPT nach. In diesem einfachen Beispiel schaffen wir es lediglich, richtige Wörter zu generieren. Das diese dann tatsächlich sinnvoll (\"intelligent\"?) sind, ist nochmal eine ganze andere Hausnummer. \n",
    "Eine anschauliche Metapher ist, dass das Netzwerk \"halluziniert\" oder vor sich hin brabbelt. Es kommen zwar meistens an sich richtige oder aus Sicht des Netzwerks wahrscheinliche Wörter heraus - Die Wortketten sind dann aber noch sinnfrei bzw. folgen keinem erkennbaren Muster.\n",
    "\n",
    "> In diesem Beispiel machen wir es uns relativ schwer, weil wir im Gegensatz zum vorherigen Beispiel keine ganzen Wörter mehr als Token nutzen, sondern einzelne Buchstaben! Das macht es für ein Netzwerk natürlich viel schwieriger, einen verständlichen Text zu generieren. Mal sehen, ob wir es trotzdem hinbekommen, sinnvolle Zeichen-/Wortketten zu generieren!\n",
    "\n",
    "Im Vorfeld haben wir ein einfaches LSTM-Modell auf zwei verschiedene Textkorpora trainiert. Der eine Textkorpus enthält 3 Bücher von Jane Austen, die praktischerweise im Paket `janeaustenr` vorliegen. Der andere Textkorpus enthält alle Songtexte von Taylor Swift aus dem Paket `taylor`.\n",
    "\n",
    "|                     | Jane Austen                                         | Taylor Swift   |\n",
    "|----------           |----------                                           |----------      |\n",
    "| R-Paket             | `janeaustenr`                                       | `taylor`       |\n",
    "| Inhalt              | Pride & Prejudice\", Sense & Sensibility, Persuasion | Alle Songtexte |\n",
    "| # sequences         | 456.246                                             | 124.888        |\n",
    "| # unique characters | 43                                                  | 33             |\n",
    "\n",
    "Beide Textkorpora wurden im gleichen Netzwerk (wieder ein einfaches Netzwerk mit einem einzigen LSTM-Layer) trainiert. Der Einfachheit halber wurden die Modelle bereits vortrainiert (weil das Training auf code.min bis zu 2 Stunden gedauert hat, wäre es unpraktisch, das in der Übung zu machen). Ihr müsst also nicht den gesamten Code selber ausführen, wir wollen eher nochmal einen Überblick darüber bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef86a9c-62c3-4af2-ac05-cfd66765056f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ef86a9c-62c3-4af2-ac05-cfd66765056f",
    "outputId": "4ca5f552-e0b4-4771-9a36-473ad23c63c9"
   },
   "outputs": [],
   "source": [
    "pacman::p_load(\n",
    "    keras,\n",
    "    tensorflow,\n",
    "    tokenizers,\n",
    "    tidyverse,\n",
    "    tidytext,\n",
    "    janeaustenr,\n",
    "    taylor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbcd8e-2716-4ea8-a353-7ba156718b08",
   "metadata": {
    "id": "c4bbcd8e-2716-4ea8-a353-7ba156718b08"
   },
   "source": [
    "## `janeaustenr`\n",
    "\n",
    "<a title=\"James Andrews, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Jane_Austen_1870_cropped.jpg\"><img width=\"256\" alt=\"Jane Austen 1870 cropped\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/f9/Jane_Austen_1870_cropped.jpg\"></a>\n",
    "\n",
    "Als erstes schauen wir uns ganz kurz nochmal die Texte an, auf denen unser einfaches Beispiel trainiert wurde. Die Texte von Jane Austen können wir mit der Funktion `austen_books()` aus dem Paket `janeaustenr` laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac462e4c-8e4e-4ea8-bdee-53ca8be10d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "ac462e4c-8e4e-4ea8-bdee-53ca8be10d0f",
    "outputId": "6b0383e8-d48a-4fe6-e540-1a1b856bb224"
   },
   "outputs": [],
   "source": [
    "austen_books() %>% head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a76a5a-ad02-49e5-9f10-e0c32066e20f",
   "metadata": {
    "id": "c0a76a5a-ad02-49e5-9f10-e0c32066e20f"
   },
   "source": [
    "Der Text liegt in einfacher Form vor. Jede Zeile des Datensatzes enthält eine Sequenz. Die Sequenzen sind jeweils gleich lang. Um den Datensatz später im Netzwerk zu verwenden, müssen wir den Text zunächst bereinigen und dann später gleichmäßig lange Sequenzen erzeugen.\n",
    "\n",
    "Schauen wir uns die Textbereinigung kurz mal an. In einer der früheren Übungen hatten wir ja bereits etwas Textbereinigung gesehen. In diesem konkreten Fall ist unser Ziel ein einzelner, extrem langer eindimensionaler String, in dem alle Zeichen hintereinander enthalten sind. Dafür müssen wir folgende Schritte durchgehen.\n",
    "\n",
    "* Wir filtern den Textkorpus auf die 3 Bücher \"Pride & Prejudice\", \"Sense & Sensibility\" und \"Persuasion\". Wenn wir alle Bücher von Jane Austen nehmen würden, schafft der Server das Training nicht (aus Performancegründen).\n",
    "* Mit `pull()` ziehen wir nur die Textspalte aus dem Datensatz.\n",
    "* `str_c(collapse = \" \")` erzeugt einen einzelnen langen String aus allen Büchern.\n",
    "* Mithilfe von `str_remove_all()` können wir bestimmte Zeichen aussortieren. Dadurch wird eine Dimension (die Anzahl der einzigartigen Zeichen) des Netzwerkes kleiner und das Training performanter.\n",
    "* `tokenize_characters()` aus dem Paket `tokenizers` erzeugt uns pro Zeichen einen einzelnen Token (mit diesen wird das Netzwerk später trainiert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785be681-2f42-408f-8ffe-704c9e5a415d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "785be681-2f42-408f-8ffe-704c9e5a415d",
    "outputId": "14f64b96-647f-4d7f-fb09-7d2ac7990003"
   },
   "outputs": [],
   "source": [
    "text_jane <- austen_books() %>%\n",
    "\n",
    "    # damit der Server nicht abstürzt, beschränken wir die Trainingsdaten auf 3 Bücher\n",
    "    filter(book %in% c(\"Pride & Prejudice\", \"Sense & Sensibility\", \"Persuasion\")) %>%\n",
    "\n",
    "    pull(text) %>% # Extract the 'text' column from the dataset\n",
    "\n",
    "    str_c(collapse = \" \") %>% # Concatenate all text into a single string\n",
    "\n",
    "    str_remove_all(pattern = \"\\\\d\") %>% # remove numbers, from stringr\n",
    "    str_remove_all(pattern = \"£\") %>% # remove British Pound sign, from stringr\n",
    "\n",
    "    # tokenize the text into characters\n",
    "    tokenize_characters(\n",
    "        lowercase = TRUE, # convert everything to lowercase\n",
    "        strip_non_alphanum = FALSE, # we want to keep punctuation\n",
    "        simplify = TRUE\n",
    "    )\n",
    "\n",
    "text_jane %>% glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefff046-2292-42f5-ae21-015d5e7768fa",
   "metadata": {
    "id": "cefff046-2292-42f5-ae21-015d5e7768fa"
   },
   "source": [
    "Wie wir sehen können haben wir den Text jetzt in sehr einfacher Form als einen ziemlich langen eindimensionalen Vektor vorliegen. Damit das Netzwerk weiß, welche Zeichen es im Text überhaupt gibt, müssen wir zusätzlich noch einen Vektor erstellen, der alle einzigarten Zeichen des Textes enthält. Dieses \"Vokabular\" definiert, welche einzigartigen Zeichen (oder Tokens) das Modell erkennt.\n",
    "\n",
    "Das machen wir mit `unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053e3f6-213a-4746-89e0-dce3844d066b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f053e3f6-213a-4746-89e0-dce3844d066b",
    "outputId": "65ea58cd-a3fb-4b0d-ab3d-664fb039a8f8"
   },
   "outputs": [],
   "source": [
    "chars_jane <- text_jane %>%\n",
    "    unique() %>%\n",
    "    sort()\n",
    "\n",
    "chars_jane %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184b213-7651-48c5-a4b9-506120ef76bf",
   "metadata": {
    "id": "c184b213-7651-48c5-a4b9-506120ef76bf"
   },
   "source": [
    "## `taylor`\n",
    "\n",
    "<a title=\"Raph_PH, CC BY 2.0 &lt;https://creativecommons.org/licenses/by/2.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:HAIMO2210722_(30_of_51)_(52232595478)_Cropped.jpg\"><img width=\"256\" alt=\"HAIMO2210722 (30 of 51) (52232595478) Cropped\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/HAIMO2210722_%2830_of_51%29_%2852232595478%29_Cropped.jpg/256px-HAIMO2210722_%2830_of_51%29_%2852232595478%29_Cropped.jpg\"></a>\n",
    "\n",
    "Beim zweiten Textkorpus gehen wir analog vor. Diesmal benutzen wir das Paket `taylor`, welches einige Informationen über das Werk von Taylor Swift enthält. Da die Daten in anderer Form vorliegen, müssen wir ein wenig anders vorgehen. Da wir diese Woche aber nicht so viel Zeit haben, können wir leider nicht auf alles im Detail eingehen. Im originalen Datensatz `taylor_album_songs` liegen die einzelnen Lieder als verschachtelte Listen vor. Diese können wir mit `unnest()` und `unnest_tokens()` aber in die gleiche Form bringen wie vorher.\n",
    "\n",
    "Danach benutzen wir wieder die gleiche Abfolge von `pull()` & `str_c(collape = \" \")` um einen langen String zu erzeugen, entfernen mit `str_remove_all()` ungewünschte Zeichen und erstellen Tokens mit `tokenize_characters()`. Auch in diesem Fall müssen wir eine Liste mit einzigartigen Zeichen erzeugen, um die eine Dimension des Netzwerkes festzulegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35994e3-6861-49c4-bbc5-0a7f23c7daeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f35994e3-6861-49c4-bbc5-0a7f23c7daeb",
    "outputId": "10f6cf70-7eb9-485b-8ca5-b9029a0daf5d"
   },
   "outputs": [],
   "source": [
    "text_taylor <- taylor_album_songs %>%\n",
    "    unnest(lyrics) %>%\n",
    "    unnest_tokens(word, lyric) %>% # from tidytext\n",
    "    pull(word) %>%\n",
    "    str_c(collapse = \" \") %>%\n",
    "    str_remove_all(pattern = \"\\\\d\") %>%\n",
    "\n",
    "    # Tokenize the text into characters, keeping the original case and special characters\n",
    "    tokenize_characters(\n",
    "        lowercase = TRUE,\n",
    "        strip_non_alphanum = FALSE,\n",
    "        simplify = TRUE\n",
    "    )\n",
    "\n",
    "chars_taylor <- text_taylor %>%\n",
    "    unique() %>%\n",
    "    sort()\n",
    "\n",
    "text_taylor %>% glimpse()\n",
    "chars_taylor %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f14026-59cb-4001-8091-6807096b7ac1",
   "metadata": {
    "id": "88f14026-59cb-4001-8091-6807096b7ac1"
   },
   "source": [
    "## Sequenzen erzeugen\n",
    "\n",
    "Als nächstes müssen wir aus unserer Wortliste, die ja gerade nur aus einzelnen Zeichen besteht, Sequenzen erstellen. Diese Sequenzen sollen eine Länge von 40 Zeichen pro Sequenz haben, was wir einmal in der Variable `max_length` definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910f025-df2d-4e90-805c-397db477e595",
   "metadata": {
    "id": "4910f025-df2d-4e90-805c-397db477e595"
   },
   "outputs": [],
   "source": [
    "max_length <- 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dd917-a24d-489b-8ab4-c75e32da96e7",
   "metadata": {
    "id": "606dd917-a24d-489b-8ab4-c75e32da96e7"
   },
   "source": [
    "Den folgenden Code werden wir allerdings nicht ausführen, da das Training des Netzwerkes leider zu lange dauern würde. Ihr könnt euch die folgenden Schritte trotzdem einmal anschauen und versuchen, nachzuvollziehen.\n",
    "\n",
    "### Wo stehen wir jetzt?\n",
    "\n",
    "Wir haben zwei Textkorpora, die jeweils als seeeehr lange Liste von einzelnen Zeichen (aber in der richtigen Reihenfolge!) vorliegen. Damit kann ein LSTM-Netzwerk noch nicht besonders viel anfangen. Die Idee ist jetzt, dass wir längere Zeichensequenzen erstellen und dazugehörig zu jeder Sequenz das Zeichen wählen, welches danach folgt. Ein einfaches Beispiel:\n",
    "\n",
    "> Wenn wir die Sequenz \"Guten Morge\" haben, dann würden wir in den meisten Fällen ein \"n\" nachfolgen lassen, damit die Phrase \"Guten Morgen\" entsteht.\n",
    "\n",
    "Das ist natürlich ein sehr einfaches und klares Beispiel. Schwieriger wird es, wenn es mehrere Möglichkeiten gibt. Um dem Netzwerk zu helfen, haben wir oben eine Sequenzlänge von 40 definiert. Dadurch sollte es besser in der Lage sein, richtige Wörter zu bilden. 40 Zeichen pro Sequenz sind lang genug, um möglichst sinnvolle Phrasen zu bilden, aber wiederum nicht zu lang, sodass das Netzwerk noch performant trainiert werden.\n",
    "\n",
    "> Das Netzwerk bekommt als Input also eine Wortsequenz der Länge 40 und lernt dann, welches Zeichen auf diese Wortsequenz folgt. Wird dieser Prozess vielfach wiederholt, kann es einen Text generieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29268b5-3f6d-45ef-ad7b-2d4d74fc341d",
   "metadata": {
    "id": "b29268b5-3f6d-45ef-ad7b-2d4d74fc341d"
   },
   "source": [
    "Zunächst müssen wir die notwendigen **Sequenzen erzeugen**. Das machen wir relativ trivial. Wir nehmen ein \"Fenster\" von einer Länge von 40 und \"verschieben\" das über den gesamten String, jeweils im Abstand von 4 Zeichen pro Schritt. Beim Taylor-Datensatz verschieben wir es im Abstand von 3 Zeichen pro Schritt. Weil dieser kleiner ist, können wir relativ zur gesamten Zeichenanzahl mehr Sequenzen erzeugen.\n",
    "\n",
    "* Wie wichtig ist dieser gewählte Abstand für die Performance des Netzwerkes? Diskutiert dabei auch kurz, wie sich die Qualität des Netzwerkes verändern könnte, wenn hier größere oder kleinere Werte gewählt werden.\n",
    "\n",
    "Die Indexierung der Sequenzen wird mithilfe von `seq()` erstellt, und anschließend erzeugen wir pro Index eine Liste mit zwei Elementen. Das erste Element ist die Textsequenz der Länge 39 (`max_length`- 1), welches in der Spalte `sentence` gespeichert wird. Das zweite Element ist das zugehörige Zeichen, welches an 40. Stelle folgt, und heißt `next_char`!\n",
    "\n",
    "> Diesen Code müsst ihr nicht ausführen: ihr ladet später ohnehin das trainierte Modell. Es folgen jetzt ein paar Codeblöcke zum Durchlesen, die ihr aber nicht ausführen müsst!\n",
    "\n",
    "```\n",
    "##### create sequences by sliding a window of size `max_length` over the text #####\n",
    "\n",
    "dataset <- map(\n",
    "\n",
    "    seq(                                           # generate a sequence of starting indices\n",
    "        1,\n",
    "        length(text) - max_length - 1,             # calculate the amount of sequences from the length of the text corpus\n",
    "        by = 4                                     # step 4 characters each time\n",
    "    ),\n",
    "\n",
    "    # For each starting index, create a list with two elements:\n",
    "    ~list(\n",
    "        sentence = text[.x:(.x + max_length - 1)], # a substring of `text` from the current index to (`max_length` - 1) characters ahead\n",
    "        next_char = text[.x + max_length]          # the character immediately following the end of the 'sentence'\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset <- transpose(dataset)                      # Convert the list of lists into a data frame where each row is a list element\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee7793-4acf-4320-8ff9-b4d260332290",
   "metadata": {
    "id": "1dee7793-4acf-4320-8ff9-b4d260332290"
   },
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Anschließend wird es noch etwas verwirrender. Wie im ersten Klassifizierungsbeispiel versteht das LSTM-Netzwerk zunächst erstmal keinen Text. Es kann nur mit numerischen Daten arbeiten. D.h., wir müssen etwas tricksen, und unsere Wortketten wieder in numerische Vektoren umwandeln. Wir müssen also wieder **one-hot encoding** anwenden.\n",
    "\n",
    "Um das Modell zu trainieren, erzeugen wir zwei Vektoren. `x` enthält die Wortsequenzen, und `y` enthält die Zeichen, welche auf die Wortsequenzen folgen (also analog zu `sentence` und `next_char` in der Zelle zuvor). Diesmal aber one-hot encodet! Jedes einzelne Element von `x` ist eine Matrix, und jedes einzelne Element von `y` ein Vektor. Diesmal wird allerdings nicht der Häufigkeitsindex kodiert, sondern jedes einzelne Zeichen aus unserem Vokabular hat seine eigene Dimension.\n",
    "\n",
    "> Beispiel: Wenn unser Vokabular die Zeichen \"a\", \"b\" und \"c\" enthält, dann haben wir drei Zeichen und somit drei Dimensionen. Das Zeichen \"a\" wird auf die erste Dimension enkodiert und bekommt den Vektor (1, 0, 0) zugewiesen, das Zeichen \"b\" auf die zweite Dimension und bekommt den Vektor (0, 1, 0), usw...\n",
    "\n",
    "Mit einer for-Schleife wird dann über jede Sequenz iteriert, und die Zeichenkette wird für `x` in eine Matrix, und für `y` in einen Vektor umgewandelt. Die beiden entstehenden Listen speichern wir in der Variable `vectors`.\n",
    "\n",
    "```\n",
    "##### one-hot encoding #####\n",
    "\n",
    "x <- array(                                                            # initialize an array for input sequences\n",
    "        0,\n",
    "        dim = c(length(dataset$sentence), max_length, length(chars))   # with dimensions (# sequences) x (max_length) x (# unique characters)\n",
    "    )\n",
    "\n",
    "y <- array(                                                            # initialize an array for output\n",
    "        0,\n",
    "        dim = c(length(dataset$sentence), length(chars))               # with dimensions (# sequences) x (# unique characters)\n",
    "    )\n",
    "\n",
    "for(i in 1:length(dataset$sentence)){                                  # Iterate over each sequence in the dataset\n",
    "\n",
    "    x[i,,] <- sapply(chars, function(x){                               # One-hot encode each character in the sentence\n",
    "        as.integer(x == dataset$sentence[[i]])\n",
    "    })\n",
    "\n",
    "    y[i,] <- as.integer(chars == dataset$next_char[[i]])               # One-hot encode the next character after the sentence\n",
    "}\n",
    "\n",
    "vectors <- list(y = y, x = x)                                          # one-hot encoded input (x) and output (y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a9097-fa38-4ac2-8378-e15314009852",
   "metadata": {
    "id": "c01a9097-fa38-4ac2-8378-e15314009852"
   },
   "source": [
    "Jetzt haben wir den komplizierten Teil größtenteils geschafft. Im nächsten Schritt definieren wir wie weiter oben und letzte Woche unser Modell/Netzwerk.\n",
    "\n",
    "* Wir benutzen ein sequenzielles Modell mit `keras_model_sequential()`.\n",
    "* Der erste Layer ist vom Typ `layer_lstm()` besteht aus 128 Einheiten und hat als Input-Dimensionen `max_length` und die Anzahl der einzigartigen Zeichen.\n",
    "* Anschließend folgt ein Output-Layer vom Typ `layer_dense()` mit einer softmax-activation. Dieser hat so viele Einheiten, wie es einzigartige Zeichen gibt. Durch die softmax-activation erreichen wir, dass das Netzwerk pro eingelesener Sequenz nur einen Buchstaben ausspuckt (wir wollen pro Sequenz ja auch nur das nächste Zeichen ausspucken).\n",
    "* An letzter Stelle wird das Modell kompiliert (hier können wir verschiedene Methoden auswählen, das sei an dieser Stelle aber nicht so wichtig).\n",
    "\n",
    "```\n",
    "##### model definition #####\n",
    "\n",
    "model <- keras_model_sequential() %>%                     # choose a sequential model\n",
    "\n",
    "  \n",
    "    layer_lstm(                                           # add an LSTM layer with 128 units\n",
    "        128,\n",
    "        input_shape = c(max_length, length(chars))        # the input shape is defined by the max_length of sequences and the number of unique characters\n",
    "    ) %>%  \n",
    "\n",
    "  layer_dense(length(chars)) %>%                          # add a densely connected output layer with a number of units equal to the number of unique characters\n",
    "\n",
    "  layer_activation(\"softmax\") %>%                         # add a softmax activation layer to output probabilities for each character\n",
    "\n",
    "  compile(                                                # compile the model\n",
    "      loss = \"categorical_crossentropy\",\n",
    "      optimizer = \"adam\"\n",
    "  )\n",
    "\n",
    "summary(model)\n",
    "```\n",
    "\n",
    "Eine `summary()` des Modells sieht dann folgendermaßen aus:\n",
    "\n",
    "* Welche Informationen erhaltet ihr aus der `summary()`?\n",
    "\n",
    "```\n",
    "# Model: \"sequential\"\n",
    "# ___________________________________________________________________________\n",
    "#  Layer (type)                       Output Shape                    Param #     \n",
    "# ===========================================================================\n",
    "#  lstm (LSTM)                        (None, 128)                     88064       \n",
    "#  dense (Dense)                      (None, 43)                      5547        \n",
    "#  activation (Activation)            (None, 43)                      0           \n",
    "# ===========================================================================\n",
    "# Total params: 93611 (365.67 KB)\n",
    "# Trainable params: 93611 (365.67 KB)\n",
    "# Non-trainable params: 0 (0.00 Byte)\n",
    "# ___________________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83771063-912a-4532-bdf7-f24a14b84c0f",
   "metadata": {
    "id": "83771063-912a-4532-bdf7-f24a14b84c0f"
   },
   "source": [
    "## Model training\n",
    "\n",
    "Jetzt, wo wir Trainingsdaten und das Modell definiert haben, können wir es mit `fit()` trainieren. Als Inputs übergeben wir die beiden oben erstellten, one-hot encodeten Vektoren `vectors$x$` (die Sequenzen) und `vectors$y` (die Zeichen, die auf die Sequenzen folgen). Damit der Server nicht abschmiert, wählen wir eine eher kleine `batch_size` (die Anzahl der Sequenzen, die gleichzeitig ins Netzwerk geschickt werden), sowie eine kleine Zahl von 10 Trainingsrunden.\n",
    "\n",
    "Nach jeder Ausführung des Codes wird das Modell mit der Funktion `save_model_hdf5()` gespeichert, sodass ihr jetzt gleich direkt auf die trainierten Modell zugreifen könnt!\n",
    "\n",
    "```\n",
    "model %>% fit(\n",
    "    vectors$x, vectors$y,\n",
    "    batch_size = 128 * 10,\n",
    "    epochs = 10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf1dab-862f-42b8-99a9-42adba365bfe",
   "metadata": {
    "id": "dbbf1dab-862f-42b8-99a9-42adba365bfe"
   },
   "source": [
    "## Generate a text\n",
    "\n",
    "Was machen wir jetzt mit den trainierten Modellen? Wir möchten sie auf ihre Fähigkeit hin vergleichen, Text zu generieren. Dafür müssen wir uns allerdings noch einige Funktionen definieren.\n",
    "\n",
    "Weiter oben, während des one-hot encodings, hatten wir ja bereits unsere Sequenzen von Text in Matrixform übersetzt. Diese Übersetzung definieren wir an dieser Stelle nochmal als eigene Funktion `convert_sentence_to_data()`. Im Prinzip wird hier wieder eine Sequenz Zeichen für Zeichen numerische Vektoren übersetzt, und diese werden dann als 3-dimensionale Array zurückgegeben, sodass das Modell sie als Input akzeptiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a3a1c-fdb6-4bd7-8723-9fe45e9e0850",
   "metadata": {
    "id": "f10a3a1c-fdb6-4bd7-8723-9fe45e9e0850"
   },
   "outputs": [],
   "source": [
    "convert_sentence_to_data <- function(sentence, chars){   # Function to convert a sentence into a numeric data format\n",
    "\n",
    "    x <- sapply(chars, function(x){                      # Convert each character in the sentence into a numeric format\n",
    "        as.integer(x == sentence)                        # Create a binary vector where 1 represents a match with the current character\n",
    "    })\n",
    "\n",
    "    array_reshape(x, c(1, dim(x)))                       # Reshape the binary matrix into a 3-dimensional array suitable for model input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d4e74-f5d6-4e2a-94c0-39881edc42e6",
   "metadata": {
    "id": "686d4e74-f5d6-4e2a-94c0-39881edc42e6"
   },
   "source": [
    "### Temperature function\n",
    "\n",
    "Bei der Generierung von Text und Bild wird im Deep Learning häufig eine sogenannte **Temperature function** hinter den Output-Layer geschaltet. Die Temperature function ist eine mathematische Funktion, welche so gewählt ist, dass die die Zufälligkeit des Outputs steuern kann (Vielleicht erinnert ihr Euch an das Glücksrad aus der Vorlesung mit Chris Biemann). Eine **höher gewählte Temperatur** führt zu einer Verteilung mit höherer Entropie, was wiederum **zufälligeren/unstrukturierteren** und dadurch potentiell überraschenden oder spannenden Output generiert. Eine **niedriger gewählte Temperatur** resultiert in **strukturierteren** Daten. Niedrigere Temperaturen lassen das Modell konservativer agieren, höhere Temperaturen führen zu einer größeren Exploration des Modells.\n",
    "\n",
    "> Higher temperatures result in sampling distributions of higher entropy that will generate more surprising and unstructured generated data, whereas a lower temperature will result in less randomness and much more predictable generated data (*Chollett et. al. 2022*)\n",
    "\n",
    "Der folgende Code erzeugt eine Funktion, welche den Output des neuronalen Netzwerks mit einer extern gewählten Temperatur zwischen 0 und 1 verrechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a34ef-3baf-4e54-951b-372b3d88b85c",
   "metadata": {
    "id": "4f1a34ef-3baf-4e54-951b-372b3d88b85c"
   },
   "outputs": [],
   "source": [
    "choose_next_char <- function(preds, chars, temperature){         # Function to choose the next character in a sequence based on model predictions\n",
    "\n",
    "    # Adjust the prediction probabilities using the temperature parameter\n",
    "    preds <- log(preds) / temperature\n",
    "    exp_preds <- exp(preds)\n",
    "\n",
    "    preds <- exp_preds / sum(exp_preds)                          # Normalize the adjusted probabilities so they sum to 1\n",
    "\n",
    "    next_index <- rmultinom(1, 1, preds) %>%                     # Sample from the adjusted probabilities to choose the next character index\n",
    "        as.integer() %>%\n",
    "        which.max()\n",
    "\n",
    "    chars[next_index]                                            # Return the character corresponding to the chosen index\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56786460-417d-4679-b13e-f9f380341001",
   "metadata": {
    "id": "56786460-417d-4679-b13e-f9f380341001"
   },
   "source": [
    "### Generator function\n",
    "\n",
    "Um Text einfach generieren zu können, müssen wir die oberen Schritte jetzt noch kombinieren. Die Funktion `generate_text()` erhält als Input:\n",
    "\n",
    "* das gewünschte Modell `model`\n",
    "* den Textkorpus `text`\n",
    "* das zur Verfügung stehende Vokabular `chars`\n",
    "* die Länge des zu generierenden Textes `length`\n",
    "* sowie die Temperatur `temperature`\n",
    "\n",
    "Anschließend wählt es mithilfe von `sample()` eine zufällige Sequenz aus dem Text aus. Das ist der Startpunkt des Modells. Es folgt ein For-Loop: dieser wiederholt sich so oft, wie Zeichen generiert werden sollen.\n",
    "\n",
    "* Die momentane Sequenz `sentence` wird mithilfe von `convert_sentence_to_data()` in Matrixform enkodiert.\n",
    "* Die nun enkodierte Sequenz wird vom Modell `model` mithilfe von `predict()` vorhergesagt.\n",
    "* Die Temperature function `choose_next_char()` generiert in Abhängigkeit von der Temperatur das nächste Zeichen.\n",
    "* Dieses wird dem String `generated` angehängt und die Sequenz wird um ein Zeichen nach links verschoben (das generierte Zeichen wird angehängt und das erste Zeichen der Sequenz fliegt raus).\n",
    "\n",
    "Wenn wir diesen Vorgang öfter wiederholen, sollte das Modell einen längeren Text generieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4733ae-921f-47ff-a9d2-2bf42189130f",
   "metadata": {
    "id": "3f4733ae-921f-47ff-a9d2-2bf42189130f"
   },
   "outputs": [],
   "source": [
    "generate_text <- function(model, text, chars, length = 100, temperature = 0.5){\n",
    "\n",
    "    start_index <- sample(1:(length(text) - max_length), size = 1)                # take a random starting index\n",
    "    sentence <- text[start_index:(start_index + max_length - 1)]                  # and select the according sequence\n",
    "    generated <- \"\"                                                               # initialize an empty string for the generated output\n",
    "\n",
    "    for(i in 1:length){                                                           # repeat as many times as stated by the user\n",
    "\n",
    "        sentence_data <- convert_sentence_to_data(sentence, chars)                # encode to matrix form\n",
    "        preds <- predict(model, sentence_data)                                    # get the predictions for each next character\n",
    "        next_char <- choose_next_char(preds, chars, temperature = temperature)    # choose the character with the temperature function\n",
    "\n",
    "        generated <- str_c(generated, next_char, collapse = \"\")                   # add it to the generated text\n",
    "        sentence <- c(sentence[-1], next_char)                                    # and continue by shifting the sequence by 1 to the left\n",
    "    }\n",
    "\n",
    "    model_path = deparse(substitute(model))\n",
    "    cat(                                                                          # print information about the current run\n",
    "        \"\\n\", \"text:\", str_extract(model_path, \"(?<=lstm_models/)[^_]+\"),         # print the text base of the model\n",
    "        \"/ number of epochs:\", str_extract(model_path, \"..(?=epochs)\"),             # print the number of epochs\n",
    "        \"/ temperature:\", temperature, \"\\n\\n\",                                    # print the chosen temperature\n",
    "        generated                                                                 # print the generated text\n",
    "       )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1df6e-8aba-4f16-9f71-5166a30197fa",
   "metadata": {
    "id": "a7c1df6e-8aba-4f16-9f71-5166a30197fa"
   },
   "source": [
    "## Text generieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00398776-fe6f-492d-8d3b-53e76bb14b81",
   "metadata": {
    "id": "00398776-fe6f-492d-8d3b-53e76bb14b81"
   },
   "source": [
    "Jetzt können wir die vortrainierten Modelle laden und miteinander vergleichen. Diese liegen im Ordner `data/lstm_models/`. Die Modelle können mit der `keras`-Funktion `load_model_hdf5()` geladen werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1252f7b-8d0e-4a63-9cf0-f2770411f766",
   "metadata": {
    "id": "b1252f7b-8d0e-4a63-9cf0-f2770411f766"
   },
   "source": [
    "* Vergleicht im Folgenden verschiedene Parameter. Wie unterscheiden sich die generierten Texte\n",
    "    * je nach Datengrundlage (Jane vs. Taylor)?\n",
    "    * je nach Anzahl der durchlaufenen Epochen?\n",
    "    * je nach verschiedenen Werten für `temperature`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ce461-ad55-48ac-94b1-d5f572f6af96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "7f9ce461-ad55-48ac-94b1-d5f572f6af96",
    "outputId": "40ae3a6f-563e-4f51-dc3e-afa178766f23"
   },
   "outputs": [],
   "source": [
    "# Beispiel\n",
    "generate_text(model = load_model_hdf5(\"data/lstm_models/jane_40epochs.h5\"), text = text_jane, chars = chars_jane, length = 500, temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce22136-1340-4e77-badc-32a520c0cfe0",
   "metadata": {
    "id": "5ce22136-1340-4e77-badc-32a520c0cfe0",
    "outputId": "5fed78b3-571b-4587-d8d5-9919b3d1bfb5"
   },
   "outputs": [],
   "source": [
    "# Beispiel\n",
    "generate_text(model = load_model_hdf5(\"data/lstm_models/taylor_10epochs.h5\"), text = text_taylor, chars = chars_taylor, length = 500, temperature = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4b6eb-7f05-4fa1-81ac-ae0319e0d9bf",
   "metadata": {
    "id": "adc4b6eb-7f05-4fa1-81ac-ae0319e0d9bf",
    "outputId": "b73d8c95-9b5b-4af2-b520-cc430d073cdd"
   },
   "outputs": [],
   "source": [
    "# Beispiel\n",
    "generate_text(model = load_model_hdf5(\"data/lstm_models/taylor_40epochs.h5\"), text = text_taylor, chars = chars_taylor, length = 500, temperature = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddb447-15d8-49d8-ad78-fae6d42080a1",
   "metadata": {
    "id": "49ddb447-15d8-49d8-ad78-fae6d42080a1"
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Die generierten Texte sind natürlich noch relativ basic. Das Modell kann nicht wirklich sinnvolle Phrasen erzeugen. Auch die Datengrundlage ist natürlich sehr klein. Aber immerhin schafft es in den meisten Fällen, korrekte englische Wörter zu bilden! Dafür, dass wir ein sehr einfaches LSTM-Netzwerk mit nur einem inneren Layer und einem kleinen Textkorpus gebaut haben, ist das schon gar nicht schlecht!\n",
    "\n",
    "\n",
    "|                 | MNIST       | Taylor    | Jane      | GPT-4         | Faktor         |\n",
    "|----------       |----------   |---------- |---------- |----------     |----------      |\n",
    "| Layers          | 4           | 2         | 2         | 120           | $60$           |\n",
    "| Parameters      | 1.8 million | 86.560    | 93.611    | ~1.8 trillion | $\\approx 10^7$ |\n",
    "| Training tokens | 60.000      | 124.874   | 456.236   | ~13 trillion  | $\\approx 10^8$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df77342-0e86-492d-a838-2ac30da5967c",
   "metadata": {
    "id": "0df77342-0e86-492d-a838-2ac30da5967c",
    "outputId": "171b748f-daff-4100-d240-38ad2fa89ebd"
   },
   "outputs": [],
   "source": [
    "summary(load_model_hdf5(\"data/lstm_models/taylor_40epochs.h5\"))\n",
    "summary(load_model_hdf5(\"data/lstm_models/jane_40epochs.h5\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
