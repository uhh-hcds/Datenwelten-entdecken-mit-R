{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566bc926-fef9-4e85-a193-1c132ce84ec1",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "In der 10. Situng geht es um **K-means Clustering**, ein Klassifizierungs-Algorithmus im maschinellen Lernen, der auf **Unsupervised Learning** (unüberwachten Lernen) basiert. Dadurch unterscheidet er sich von den bisher kennengelernten Modellen, die alle auf Supervised Learning beruhen. K-means Clustering versucht bei einer großen Menge unstrukturierter Daten die einzelnen Datenpunkte in **Cluster** (Gruppen) aufzuteilen. Ähnliche Datenpunkte werden dabei in demselben Cluster zusammengefasst. Die Kategorien der Datenpunkte sind vorher unbekannt, daher können wir auch kein Supervised Learning nutzen. K-means versucht dabei, natürliche Strukturen oder Muster in dem Datensatz zu entdecken und zu interpretieren.\n",
    "\n",
    "Der Algorithmus stammt ursprünglich aus den 1960er Jahren, ist also einer der früheren ML-Algorithmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25917a0-a46e-4785-9f0e-63f85b0f1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(tidyverse, tidymodels, janitor, devtools, ggrepel)\n",
    "options(repr.plot.width = 16, repr.plot.height = 8) # Jupyter display options\n",
    "set.seed(123) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57d12f-082c-4c26-b53e-50150f6bed19",
   "metadata": {},
   "source": [
    "## Wie funktioniert K-means Clustering?\n",
    "\n",
    "Die mathematischen Grundlagen für K-means werden in der Vorlesung hergeleitet. Der Algorithmus lässt sich aber relativ einfach visualisieren und wird dadurch anschaulich nachvollziehbar.\n",
    "\n",
    "1. Schaut euch die folgenden beiden Gifs an und versucht, die einzelnen Schritte des Algorithmus qualitativ zu beschreiben.\n",
    "\n",
    "Artwork by @allisonhorst\n",
    "\n",
    "<a title=\"Artwork by @allisonhorst\" href=\"https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif\"><img width=\"50%\" alt=\"K-means convergence illustrated by @allisonhorst\" src=\"https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif\"></a>\n",
    "\n",
    "<a href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\">Chire</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">CC BY-SA 4.0</a>, via Wikimedia Commons\n",
    "\n",
    "<a title=\"Chire, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:K-means_convergence.gif\"><img width=\"30%\" alt=\"K-means convergence\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/K-means_convergence.gif/512px-K-means_convergence.gif\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d5a37-9b21-44b0-bd01-80162dd5d351",
   "metadata": {},
   "source": [
    "## Einfache Einführung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59c43a42-e881-4b84-941a-55f89d6ba926",
   "metadata": {},
   "source": [
    "Zunächst werden wir mit einem sehr einfachen Beispiel anfangen. Dafür generieren wir uns selbst zweidimensionale Zufallsdaten aus drei Clustern. Die einzelnen Datenpunkte in jedem Cluster folgen einer multivariaten Gauß-Verteilung. Wir merken uns die Label der Cluster, werden diese aber nicht im Training verwenden! Der Algorithmus kennt also nur die zufällig generierten Datenpunkte **ohne** Label und versucht innerhalb dieser ein Muster zu erkennen!\n",
    "\n",
    "> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Multivariate_normal_density.png/450px-Multivariate_normal_density.png\"  alt=\"zweidimensionale Gauss-Verteilung\" width=\"150\" align=\"right\" > Eine multivariate Gauß-Verteilung (\"Normalverteilung\") ist eine Gauß-Verteilung in mehreren Dimensionen. Vielleicht hilft euch das Bild rechts (Bildquelle: <a href=\"https://de.wikipedia.org/wiki/Mehrdimensionale_Normalverteilung\">Wikipedia</a>), damit ihr euch eine Gauß-Verteilung für zwei Variablen vorstellen könnt. Die Gauß-Verteilung selbst wird in der Vorlesung ausführlich erklärt. \n",
    "\n",
    "\n",
    "### Zufällige Datenpunkte generieren\n",
    "\n",
    "Um die Datenpunkte zu generieren, erzeugen wir zunächst einen `tibble` namens `centers` aus 3 Reihen. Jede Reihe enthält die \"Metadaten\" eines Clusters, nämlich das Label `cluster`, die Anzahl der Punkte `num_points` sowie die Koordinaten `x1` und `x2` für die beiden Dimensionen.\n",
    "\n",
    "> Die Erzeugung der zufälligen Daten müsst ihr nicht in jedem Detail nachvollziehen, uns geht es ja eigentlich eher darum, wie wir dann den K-means Algorithmus darauf anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e729b7-4d30-4633-9d8a-485696de55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers <- tibble(\n",
    "    cluster = factor(1:3),         \n",
    "    num_points = c(100, 150, 50),  # number points in each cluster\n",
    "    x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n",
    "    x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n",
    "    ) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93be824-73ac-4d59-8331-eaea8b37af71",
   "metadata": {},
   "source": [
    "Anschließend müssen wir die Datenpunkte generieren! \n",
    "\n",
    "Für die beiden Dimensionen `x1` und `x2` wird mithilfe von `mutate()` und `map2()` eine Liste an zufällig erzeugten Datenpunkten generiert. `rnorm` erzeugt für uns dabei zufällige Werte, die einer Gauß-Verteilung folgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5195f-d2ff-4301-9515-536ee218595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_points <- centers %>%\n",
    "    mutate(\n",
    "        x1 = map2(num_points, x1, rnorm), # generate data points in dimension x1\n",
    "        x2 = map2(num_points, x2, rnorm)  # generate data points in dimension x2\n",
    "    ) %>% \n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6be264-6cf4-49a4-818c-adbb3c5f1968",
   "metadata": {},
   "source": [
    "Anschließend können wir mit `unnest()` die Liste **entpacken**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693373e-5b7d-43ae-b1d0-d40ce963ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_points <- labelled_points %>%\n",
    "    select(-num_points) %>% \n",
    "    unnest(cols = c(x1, x2)) %>% # unnest the data points in both dimensions\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde77f4-1169-497a-b53d-fb8f8b296729",
   "metadata": {},
   "source": [
    "So sieht es aus wenn wir die zufällig generierten Datenpunkte visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40c07e-bb04-4264-a8f2-0feea7974901",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_points %>%\n",
    "    ggplot(aes(x1, x2, color = cluster)) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a688b8f-8206-45ef-9d63-17abdb04e21b",
   "metadata": {},
   "source": [
    "Soweit so gut! Wir haben jetzt drei Cluster erzeugt und schon rein optisch lassen sich diese sehr gut trennen, wobei es auch einzelne Datenpunkte gibt, die räumlich recht weit von ihrem Cluster entfernt sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014cf07-9c93-4360-84b6-92c2c8c7d50d",
   "metadata": {},
   "source": [
    "### Clustering mit K-means\n",
    "\n",
    "Um K-means zu testen, müssen wir allerdings die Information über die Cluster entfernen! K-means soll ja nicht wissen, welcher Punkt zu welchem Cluster gehört.\n",
    "\n",
    "Das machen wir wie gewohnt mit `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceacba-c73b-470a-b438-82e0e58ae070",
   "metadata": {},
   "outputs": [],
   "source": [
    "points <- labelled_points %>% \n",
    "    select(-cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae104aa9-ee3e-40b2-af75-0593e92e6d75",
   "metadata": {},
   "source": [
    "Visualisiert sieht das Ganze dann folgendermaßen aus. \n",
    "\n",
    "Ohne die Färbung wird es direkt schon etwas schwieriger fürs menschliche Auge, Clusterstrukturen zu erkennen. Der rechte Cluster ist immer noch relativ eindeutig, aber die Datenpunkte der linken beiden Cluster verschwimmen schon etwas mehr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84969ad1-feb1-40f8-8b05-2e00f1c347cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points %>%\n",
    "    ggplot(aes(x1, x2)) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93723557-a720-4ec2-a85e-33f8e236dd12",
   "metadata": {},
   "source": [
    "K-means ist in R bereits als eigene Funktion `kmeans()` implementiert als Teil des Paketes `stats`. \n",
    "\n",
    "Die einzigen zwingenden Argumente sind der Datensatz (in unserem Fall die ungelabelten Datenpunkte `points`) sowie die Anzahl der **centers**, auch als **k-value** $k$ bekannt. Wir sind selbst dafür verantwortlich, den angemessenen Wert für $k$ festzulegen, bzw. können wir können versuchen, durch verschiedene Methoden einen guten Wert herauszufinden. K-means wählt selber die Startpunkte für den Algorithmus und versucht dann, zu konvergieren. Das $k$ steht dabei für die Anzahl der Cluster.\n",
    "\n",
    "In diesem Fall wissen wir ja, dass in den Daten drei Cluster enthalten sind - haben wir ja selbst oben so festgelegt und die Daten entsprechend generiert. Deshalb sagen wir K-means einfach direkt, dass $k = 3$ ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b455c0-6983-43be-a189-36394f141e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust <- points %>%\n",
    "    kmeans(centers = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040517b-c0a0-4bda-8ca5-3d44d3975073",
   "metadata": {},
   "source": [
    "Okay, schauen wir doch mal, was jetzt in unserem Modell gespeichert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3290d-6993-472f-a5d3-f80aa0f24baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f031983-0583-46d7-86f1-98d04374c286",
   "metadata": {},
   "source": [
    "Hui, das sind eine ganze Menge Informationen.\n",
    "\n",
    "R gibt uns eine ausführliche Zusammenfassung über die Größe der Cluster (**sizes**), die Koordinaten der gefundenen Cluster (**Cluster means**),  sowie den **Clustering vector**, der für jeden Datenpunkt den Cluster enthält, dem dieser zugeordnet wurde.\n",
    "\n",
    "Außerdem erhalten wir eine wichtige Information über die Güte des Clusterings. **Within cluster sum of squares by cluster** sagt uns nämlich aus, wie groß die \"Summe der quadrierten Abweichungen vom jeweiligen Cluster-Zerntrum\" pro Cluster ist.  Für jeden Punkt wird die Distanz zum Cluster-Zerntrum berechnet. Diese wird dann quadriert und über alle Punkte des Clusters hinweg aufsummiert. Das ist so ähnlich, wie bei der Standardabweichung.\n",
    "\n",
    "Den Wert **Within cluster sum of squares by cluster** versucht K-means zu minimieren. Eine niedrigerer Wert bedeutet, dass die Punkte im Cluster alle am Zentrum sind. Wir können über diese Zahlen also eine Einschätzung erhalten, wie kompakt die einzelnen Cluster zusammenliegen.\n",
    "\n",
    "In Klammern unter den Werten für die einzelnen Cluster befindet sich noch eine sehr relevante Information über die Güte des Modells insgesamt, nämlich **between_SS / total_SS**. \"SS\" steht hier für \"sum of squares\". Der Wert drückt aus, wieviel der Varianz in den Daten durch die Zugehörigkeit zu den Clustern erklärt wird. Ein hoher Wert spricht für ein gutes Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2c45f-fd72-4943-89f0-5abed0b84217",
   "metadata": {},
   "source": [
    "### `augment()`, `tidy()` and `glance()`\n",
    "\n",
    "Bis hierhin war das Modellieren relativ einfach. Im Gegensatz zum Supervised Learning brauchen wir uns nicht um Trainings- und Testdaten zu kümmern. Außerdem ist es ganz praktisch, dass `kmeans()` direkt in R enthalten ist. \n",
    "\n",
    "<img src=\"https://broom.tidymodels.org/logo.png\" alt=\"broom\" width=\"100\" align=\"right\" /> Allerdings haben wir auch die Möglichkeit, die Evaluation des Modells etwas zu formalisieren. Dafür enthält `tidymodels` das Paket `broom`.\n",
    "\n",
    "`broom` kennt einige Funktionen, mit denen wir komfortabel die verschiedenen Informationen unseres Modells ausgeben können. Diese werden wir in den nächsten Schritten kennenlernen.\n",
    "\n",
    "Mit `augment()` können wir z.B. die neu erzeugten Labels (also die Informationen über die vom Modell berechneten Cluster) direkt mit den alten, ungelabelten Daten verknüpfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb629c8-598f-48af-8818-cbc3d9679f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust %>% # take the cluster model\n",
    "    augment(points) %>% # match it to the data points\n",
    "    head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8016aea-abd7-4862-8a6d-96e005c37d1e",
   "metadata": {},
   "source": [
    "Das ist praktisch, wenn wir die geclusterten Daten weiterverarbeiten wollen. Mit `augment()` können wir das mit einem Einzeiler tun.\n",
    "\n",
    "Die Funktion `tidy()` gibt uns eine Zusammenfassung auf dem Level der Cluster aus. Der Output ist relativ ähnlich zu dem Fall, dass wir einfach das Modell über `kclust` aufrufen. \n",
    "\n",
    "Der Vorteil von `tidy()` ist, dass die Zusammenfassung direkt in einem `tibble` ausgegeben wird und dadurch innerhalb des Tidyverses einfacher weiterzuverarbeiten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b170a9-deb6-43f4-ab39-c4210dc8e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust %>% \n",
    "    tidy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fe7be-aaf1-4947-a63c-768f92f4e49f",
   "metadata": {},
   "source": [
    "Auf der höchsten Ebene können wir `glance()` benutzen, um eine einzeilige Zusammenfassung des Modells zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a0d75-7dea-469a-bc64-9f309e25f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclust %>% \n",
    "    glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83549d-8f98-44bc-a78f-19320827672a",
   "metadata": {},
   "source": [
    "2. Was bedeuten die verschiedenen Variablen, die ihr mit `glance()` ausgegeben bekommt? Schaut dafür in der Dokumentation nach oder fragt eine Suchmaschine/Chatbot. (`totss`, `tot.withinss`, `betweenss`, `iter`).\n",
    "\n",
    "3. Für zukünftige Übungen: was passiert, wenn ihr `augment()`, `tidy()` und `glance()` auf andere Modelle anwendet? Behaltet das mal in Kopf oder macht euch hier Notizen und probiert es beim nächsten Termin in der Wiederholung aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ba4d4-777c-4258-8cef-170ed9d22d4e",
   "metadata": {},
   "source": [
    "### Visualisierung\n",
    "\n",
    "Jetzt wollen wir das Modell nochmal visualisieren. Dafür geben wir uns zunächst die vorhergesagten Datenpunkte mit `augment()` aus und benennen die Vorhersage der Cluster in `cluster_prediction` um.\n",
    "\n",
    "Mit einem [`left_join()`](https://dplyr.tidyverse.org/reference/mutate-joins.html) können wir dann die tatsächlichen Cluster aus unserem ursprünglichen Datensatz `labelled_points` an die Daten anfügen. Zur besseren Übersichtlichkeit benennen wir auch hier die Variable um, und zwar in `cluster_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb83577-a641-437b-84e4-6560a99c851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- kclust %>%\n",
    "    augment(points) %>%\n",
    "    rename(cluster_prediction = .cluster) %>%\n",
    "    left_join(labelled_points, by = join_by(x1, x2)) %>%\n",
    "    rename(cluster_true = cluster) %>%\n",
    "    #bind_cols(truth = labelled_points$cluster) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a85d4-3a2f-4307-8080-ec1ac8d8985e",
   "metadata": {},
   "source": [
    "Jetzt können wir das Modell und den ursprünglichen Datensatz direkt vergleichen. Dafür plotten wir die Datenpunkte und übergeben den vorhergesagten Wert `prediction` als Form und den tatsächlichen Wert `truth` als Farbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c518c-4c5c-4285-9af8-9b8b65bff46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>%\n",
    "    ggplot(aes(x1, x2, color = cluster_true, shape = cluster_prediction)) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585321a-cbdc-4a5c-8fd0-523f7d0a2e95",
   "metadata": {},
   "source": [
    "4. Wie interpretiert ihr die Grafik? Welche Datenpunkte wurden vom Algorithmus falsch zugeordnet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59f3aa-1511-4e61-ae01-d238e383b6f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #efe3fd\"><h1>Präsenzteil</h1></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b0ee5-d1a0-4130-9043-8ee68cd49cee",
   "metadata": {},
   "source": [
    "## Einfache Einführung\n",
    "\n",
    "### Increasing k\n",
    "\n",
    "Die Zusammenfassungen mit `augment()`, `tidy()` und `glance()` werden erst so richtig praktisch, wenn wir sie in Kombinationen mit weiteren Tools von `dplyr` verwenden. Wir können z.B. mal annehmen, dass wir die richtige Anzahl der Cluster $k$ nicht kennen, also eine Analyse für verschiedene Werte von $k$ vornehmen wollen. \n",
    "\n",
    "Mithilfe von `map()` könnten wir dann sehr schnell über Werte von $k$, z.B. von 1 bis 9, iterieren und unsere Ergebnisse in einem einzigen, großen `tibble` abspeichern. \n",
    "\n",
    "> Die Funktion `map()` ist wie eine \"Stapelverarbeitung\": Sie wendet eine andere Funktion auf eine Reihe von Objekten an und gibt dann die durch die Funktion veränderten Objekte zurück. Für diejenigen unter euch, die bereits etwas Programmier-Erfahrung haben: Das ist wie eine Schleife nur schneller, und man kann `map()` auch in der Pipe einsetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6810d0b-0b01-447f-94ec-240504e9452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusts <- tibble(k = 1:9) %>%\n",
    "    mutate(\n",
    "        kclust = map(k, ~kmeans(points, .x)),\n",
    "        tidied = map(kclust, tidy),\n",
    "        glanced = map(kclust, glance),\n",
    "        augmented = map(kclust, augment, points)\n",
    "    ) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeabb15-1ab9-48f9-9d0a-6daee1bc69a8",
   "metadata": {},
   "source": [
    "Huch, dass sieht ja seltsam aus! gar nicht wie ein normaler Datensatz. Da sind eckige und spitze Klammern und der Datentyp ist `list`. Der Grund ist, dass die einzelnen Outputs von `tidy()`, `glance()` und `augment()` als Listen innerhalb des Datensatzes `kclusts` vorliegen. \n",
    "\n",
    "> Dass ein Datensatz in den einzelnen Zellen Listen oder sogar andere Datensätze enthalten kann, kennt man nicht unbedingt aus anderen Programmen (z.B. Excel). Es ist ein spezielles Feature des `tibble`, das sehr praktisch sein kann.\n",
    "\n",
    "Wir können die Listen mit `unnest()` entpacken und dann in eigenen Objekten speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bd177-458e-4eef-aecd-ee9f2837ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters <- kclusts %>%\n",
    "    unnest(cols = c(tidied))\n",
    "\n",
    "assignments <- kclusts %>% \n",
    "    unnest(cols = c(augmented))\n",
    "\n",
    "clusterings <- kclusts %>%\n",
    "    unnest(cols = c(glanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f995c-3315-490e-ba56-709328370e77",
   "metadata": {},
   "source": [
    "Im nächsten Schritt sehen wir jetzt die \"Magie\" von `tidymodels`. \n",
    "\n",
    "Mit einer einfachen Pipe können wir die verschiedenen Modelle für $k=1$ bis $k=9$ in einem einzigen Diagramm visualisieren. Wir erhalten relativ schnell einen guten Überblick über die verschiedenen Werte von $k$ und können das Diagramm bereits zur ersten Einschätzung eines geeigneten Wertes von $k$ nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892d7e0-6bbb-4aa7-aca2-562ffd42f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments %>%\n",
    "    ggplot(aes(x = x1, y = x2)) +\n",
    "    geom_point(aes(color = .cluster), alpha = 0.8) + \n",
    "    facet_wrap(~ k) +\n",
    "    theme_minimal(base_size = 15) +\n",
    "    geom_point(data = clusters, size = 10, shape = \"x\") # add data from tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac5109-125c-4571-b66f-b0e0fb6d83d4",
   "metadata": {},
   "source": [
    "5. Interpretiert das Diagramm. Welche Werte von $k$ haltet ihr am geeignetsten? Welche wären auch noch okay? Wie würdet ihr das begründen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddcc48-a0d0-498f-b94a-f60cfa919295",
   "metadata": {},
   "source": [
    "### Elbow-Method\n",
    "\n",
    "Im Normalfall ist es gar nicht so leicht, die \"richtige\" Anzahl an Clustern zu bestimmen. Zudem funktioniert diese optische Methode ja auch nur gut, wenn wie im Beispiel genau zwei Variablen in das Clustering einfließen. In der Praxis fließen meist viel mehr Variablen ein. K-means kennt einige Verfahren zur Bestimmung von $k$, die mathematisch mehr oder weniger fundiert sind. Dazu gehören:\n",
    "\n",
    "1. Elbow method\n",
    "2. Silhouette method\n",
    "3. Gap statistic\n",
    "\n",
    "Diese werden detaillierter z.B. in diesem [Artikel](https://uc-r.github.io/kmeans_clustering) erklärt. \n",
    "\n",
    "Beispielhaft werden wir an dieser Stelle einmal kurz auf die **Elbow method** eingehen. Diese wird auch in der Vorlesung beschrieben und hergeleitet.\n",
    "\n",
    "Weiter oben hatten wir ja auch für den Output der Funktion `glance()` ein eigenes Objekt `clusterings` erstellt. `clusterings` kommt uns jetzt zugute, weil wir für die Elbow method die **total within-cluster sum of square** (wss) anschauen können. \n",
    "\n",
    "Dieser Wert wird uns praktischerweise direkt von `glance()` ausgegeben, und zwar innerhalb von `kmeans()` benannt als `tot.withinss`. Wenn wir also `tot.withinss` gegen `k` plotten, erhalten wir folgenden Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1bb72-c1b4-4f1a-bf12-4e0642f17b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings %>%\n",
    "    ggplot(aes(k, tot.withinss)) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 25) +\n",
    "    scale_x_continuous(breaks = seq(1, 9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df49641-6c79-4816-8e11-6a27c7a4580c",
   "metadata": {},
   "source": [
    "Eine Krümmung (englisch: bend), bzw. ein sogenannter Ellbogen, symbolisiert i.d.R. einen geeigneten Wert von $k$ für eine Clusterlösung. Dieser **bend** indiziert, dass das Hinzufügen weiterer Cluster das Modell nicht verbessert. In diesem Beispiel liegt der Ellbogen bei $k=3$, was ja mit unserer vorab generierten Anzahl der Cluster übereinstimmt.\n",
    "\n",
    "Eine mathematische Herleitung dieser Aussage findet ihr z.B. unter\n",
    "\n",
    "> [Tibshirani, R., Walther, G. & Hastie. T (2000). Estimating the Number of Clusters in a Dataset via the Gap Statistic. Journal of the Royal Statistical Society, Series B, 63(2), p. 411-423.](https://hastie.su.domains/Papers/gap.pdf)\n",
    "\n",
    "In der Realität wird K-means vor allem bei großen, unstrukturierten Datenmengen eingesetzt, sodass ein Clustering nicht immer so perfekt funktioniert wie in diesem Beispiel. Es besteht dann eine große Unsicherheit dahingehend, wieviele Cluster man extrahieren soll. Dabei werden dann mathematische Verfahren zur Bestimmung von $k$, wie z.B. die Elbow method, wichtiger. Allerdings sollte man sich auch nicht ausschließlich auf die mathematischen Verfahren beziehen, sondern auch die Interpretierbarkeit der Clusterlösung im Auge behalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5a906-83df-4472-849f-d980b8cdf837",
   "metadata": {},
   "source": [
    "## Mario Kart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec29860-307f-44c5-93a7-ace8094b424c",
   "metadata": {},
   "source": [
    "<img src=\"https://aldomann.github.io/mariokart/reference/figures/logo.png\" alt=\"Mariokart\" width=\"100\" align=\"right\" />Als nächstes Beispiel haben wir einen vielleicht eher ungewöhnlichen Datensatz über Charaktere und Vehikel des Konsolenspiels Mario Kart.\n",
    "\n",
    "Die Dokumentation des Datensatzes ist unter https://aldomann.github.io/mariokart/ zu finden.\n",
    "\n",
    "Wir können den Datensatz mithilfe von `devtools::install_github()` direkt als Paket installieren und anschließend laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b551b-8b43-4355-bb16-6afa07d1ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "devtools::install_github(\"aldomann/mariokart\")\n",
    "library(mariokart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ec608-1648-4a76-a55b-98bab5610082",
   "metadata": {},
   "source": [
    "In diesem Beispiel analysieren wir den Datensatz `mk8_characters`. Dieser besteht aus 12 Variablen, wie `speed_normal`, `acceleration` oder `grip` mit verschiedenen Werten zwischen 2 und 5. Jede Zeile steht für einen Charakter, dessen Name in der Spalte `character` gespeichert ist, und jeder Charakter ist eine Gewichtsklasse `weight_class` zugeordnet.\n",
    "\n",
    "Das ganze können wir uns auch einmal anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d961713-33bb-468d-aa20-d7c75205b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(mk8_characters)\n",
    "\n",
    "mk8_characters %>% glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea04fa-13b1-4b18-bcba-6fed03ccc273",
   "metadata": {},
   "source": [
    "Im Folgenden wollen wir versuchen, die Charaktere in Gewichtsklassen zu clustern. Auch in diesem Beispiel haben wir wieder den Vorteil dass die Gewichtsklassen im Datensatz bereits gelabelt sind, wir können unser Ergebnis also gut überprüfen!\n",
    "\n",
    "Da wir $k$ noch nicht kennen, macht es Sinn, wieder dem Ansatz von oben zu folgen, und zunächst über die visuelle Analyse sowie die Elbow-Method einen geeigneten Wert für $k$ herauszusuchen. Dafür können wir im Prinzip einfach den Code von oben kopieren und ein paar kleine Änderungen vornehmen.\n",
    "\n",
    "> 1. Wir müssen die maximale Anzahl der Iterationen von `kmeans()` hochsetzen, da der Algorithmus sonst nicht konvergiert. Das bedeutet die Anzahl der Durchläufe in denen die Punkte zu den Clusterzentren zugeordnet werden reicht nicht aus, um zu einer stabielen Lösung zu kommen. Wie brauchen mehr Durchläufe, z.B. 30. Das machen wir mit dem Argument `iter.max = 30`. Theoretisch können wir hier immer einen sehr hohen Wert einstellen, dann kann es bei sehr großen Datenmengen aber dazu kommen, dass der Algorithmus sehr lange braucht, um zu konvergieren. Daher wird hier i.d.R. eher ein niedrigerer Wert eingestellt, und nur wenn der Algorithmus nicht konvergiert, wird der Wert nach und nach hochgesetzt.\n",
    "> 2. Außerdem funktioniert `kmeans()` nur mit numerischen Variablen. Die Variablen `weight_class` und `character` müssen wir also vor Anwendung von K-means mithilfe von `select()` noch aus dem Datensatz ausschließen!\n",
    "\n",
    "Ansonsten ist der Code derselbe wie oben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e55d9b-5d6a-4bd3-b957-5ec5585f1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_kclusts <- tibble(k = 1:9) %>%\n",
    "    mutate(\n",
    "        kclust = map(k, ~kmeans(mk8_characters %>% select(-weight_class, -character), .x, iter.max = 30)),\n",
    "        tidied = map(kclust, tidy),\n",
    "        glanced = map(kclust, glance),\n",
    "        augmented = map(kclust, augment, mk8_characters)\n",
    "    )\n",
    "\n",
    "mk8_clusters <- mk8_kclusts %>%\n",
    "    unnest(cols = c(tidied))\n",
    "\n",
    "mk8_assignments <- mk8_kclusts %>% \n",
    "    unnest(cols = c(augmented))\n",
    "\n",
    "mk8_clusterings <- mk8_kclusts %>%\n",
    "    unnest(cols = c(glanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bec790-80dc-4a1c-af09-2a603b0755ca",
   "metadata": {},
   "source": [
    "Das geht wieder schnell! Die Datenmengen sind natürlich auch sehr klein, aber K-means ist auch ein sehr schneller Algorithmus.\n",
    "\n",
    "Jetzt wollen wir wieder mithilfe von `facet_wrap()` die Cluster für verschiedene Werte von $k$ darstellen, und außerdem das Diagramm für die Elbow-Method ausgeben. Da der Datensatz aus 12 numerischen Variablen besteht, müssen wir uns für die zweidimensionale Darstellung 2 Variablen raussuchen, die wir plotten können. In diesem Beispiel sind es `x = speed_normal` und `y = handling_normal`.\n",
    "\n",
    "6. Was würde passieren, wenn ihr andere Variablen für `x` und `y` verwendet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e7f69-c6cb-4745-af3c-215228e1fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_assignments %>%\n",
    "    ggplot(aes(x = speed_normal, y = handling_normal)) +\n",
    "    geom_point(aes(color = .cluster), alpha = 0.8, size = 5) + \n",
    "    theme_minimal(base_size = 15) +\n",
    "    facet_wrap(~ k) +\n",
    "    geom_point(data = mk8_clusters, size = 10, shape = \"x\") # add data from tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5851d08-9472-4083-a901-9555f6365ed6",
   "metadata": {},
   "source": [
    "Für die Elbow-Method plotten wir wieder `tot.withinss` gegen `k`. \n",
    "\n",
    "7. Bei welchem $k$ seht ihr den Ellbogen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669b991-a88c-4a59-ace8-52361f6b5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_clusterings %>%\n",
    "    ggplot(aes(k, tot.withinss)) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_point(size = 5) +\n",
    "    theme_minimal(base_size = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4551e2-8a67-46f0-9f26-e942977925a9",
   "metadata": {},
   "source": [
    "Auch in diesem Beispiel scheint $k=3$ wieder ein geeigneter Wert für K-means zu sein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00469224-028c-4d9b-9950-9789cfb2fc88",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Wir erinnern uns – die ursprünglichen Daten sind ja mit der Gewichtsklasse `weight_class` gelabelt! Mal sehen, was für Gewichtsklassen das Spiel in Betracht zieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e24e0-3c2b-44cc-b720-dff46fd48653",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_characters %>%\n",
    "    distinct(weight_class) %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8355fd6c-e118-4c75-82db-7ffbb98208ef",
   "metadata": {},
   "source": [
    "Na so ein Zufall. Auch das Spiel schlägt 3 Gewichtsklassen vor, nämlich `light`, `medium` und `heavy`. Wir können das ganze auch nochmal visualisieren und das Label `weight_class` wieder als `shape` darstellen (so wie in dem Beispiel von weiter oben mit den generierten Daten).\n",
    "\n",
    "8. Geht den folgenden Code Zeile für Zeile durch, und überlegt, was jede Zeile macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36200fd3-ec74-434e-a003-c138939df64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_assignments %>%\n",
    "    filter(k == 3) %>%\n",
    "    group_by(speed_normal, handling_normal, weight_class) %>%\n",
    "    slice(1) %>%\n",
    "    ggplot(aes(x = speed_normal, y = handling_normal, label = character, shape = weight_class, color = .cluster)) +\n",
    "    geom_point(size = 5) +\n",
    "    ggrepel::geom_label_repel(seed = 1, show.legend = FALSE) +\n",
    "    theme_minimal(base_size = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fe467-70e7-4959-a3c6-99c42fa6f0da",
   "metadata": {},
   "source": [
    "9. Wie gut funktioniert das Clustering? Wurden alle Charaktere der richtigen Gewichtsklasse zugeordnet?\n",
    "10. Wie sieht das Diagramm aus, wenn ihr zwei andere Variablen gegenüber plottet? An welchen Stellen müsst ihr die Variablen austauschen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4a101-6e83-4395-b611-ae12b4f588a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk8_assignments %>%\n",
    "    filter(k == 3) %>%\n",
    "    group_by(acceleration, weight, weight_class) %>%\n",
    "    slice(1) %>%\n",
    "    ggplot(aes(x = acceleration, y = weight, label = character, shape = weight_class, color = .cluster)) +\n",
    "    geom_point(size = 5) +\n",
    "    ggrepel::geom_label_repel(seed = 1, show.legend = FALSE) +\n",
    "    theme_minimal(base_size = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1d928-a8b9-4492-aa9b-8dae85eaffb9",
   "metadata": {},
   "source": [
    "Der Mario Kart Datensatz ist ziemlich klein, hat uns aber trotzdem anschaulich gezeigt, wie K-means verschiedene Cluster (oder Klassen) innerhalb eines unstrukturierten Datensatzes finden kann.\n",
    "\n",
    "In der Realität würden wir K-means eher auf einen größeren Datensatz anwenden, der vermutlich auch viel weniger strukturiert ist als die beiden Beispiele, die wir bisher betrachtet haben. Eine Analyse eines umfangreichen, unstrukturierten Datensatzes ist aber oft nicht ganz einfach, weil selten klar ist, welche Cluster überhaupt sinnvoll sind.\n",
    "\n",
    "Nur weil ein mathematisches Modell große Datenmengen clustern kann, heißt das nicht immer, dass wir die Cluster auch sinnvoll und logisch mit qualitativen Aussagen verknüpfen können!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9855300-1079-4fbb-8e76-2af2de5dba7f",
   "metadata": {},
   "source": [
    "Diese Übung hatte bisher schon recht viel neuen Input. Im Folgenden gibt es noch ein zweites Beispiel, welches recht analog zu den Mario Kart Daten funktioniert. Falls ihr Lust habt, könnt ihr versuchen, dieses noch durchzurechnen. Der Teil ist aber optional!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125493c-05c4-4994-88a7-693effd82448",
   "metadata": {},
   "source": [
    "## iris k-means \n",
    "\n",
    "Einer der frühesten Datensätze für Klassifizierungsmethoden stammt von Fisher aus dem Jahr 1936 und beinhaltet Daten über verschiedene Arten der Schwertlilie (aka Iris).\n",
    "\n",
    "11. Ladet den Datensatz aus dem Verzeichnis `\"data/iris/iris.data\"`. Benutzt `janitor`, um die Spaltenbezeichnungen möglichst sauber zu halten. Welche Variablen sind im Datensatz vorhanden und welche davon würden wir für ein K-means Clustering verwenden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db112622-43d5-4e98-a778-28b8a36f4cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa4dd4d-469a-4bfb-a3de-c788b3a5270a",
   "metadata": {},
   "source": [
    "12. Erstellt wie in den vorangegangenen Beispielen eine Auswertung für verschiedene Werte von $k$ mithilfe der Elbow-Method. Welches $k$ ist für unser Modell am sinnvollsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fd5fe-de61-4ae8-80db-4b272d08293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4154f7c-a97c-4aca-98d6-65d676ce7c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "554bebee-b903-46e9-8d75-fa2c31361db9",
   "metadata": {},
   "source": [
    "13. Wenn ihr das richtige $k$ gefunden habt, erstellt eine grafische Auswertung des Modells. Ihr könnt beispielsweise `sepal_length` auf der x-Achse gegen `sepal_width` auf der y-Achse plotten und die `prediction` als Farbe sowie den tatsächlichen Wert `species` als Form übergeben. Welche Datenpunkte wurden falsch zugeordnet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e40893-90ec-480e-a4ed-dec01e9a540c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17fc1120-72b0-41c3-941e-a51407e85964",
   "metadata": {},
   "source": [
    "14. Wie gut funktioniert K-means in diesem Beispiel im Vergleich zu den vorhergehenden Beispielen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01095cc4-99c4-4d9e-b9f0-f308e9e2425a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "> K-means clustering ist ein sehr einfacher und schneller Algorithmus. Dadurch kann K-means effizient mit sehr großen unstrukturierten Datenmengen umgehen. Eine Herausforderung von K-means ist, dass eine menschliche Eingabe über die Anzahl der Cluster benötigt wird und dass eine Unsicherheit über die Wahl des \"richtigen\" $k$ besteht. Andere Modelle, die man auch zum Clustern verwenden kann, wie z.B. Decision Trees, bilden die Gruppen automatisch. Ein weiterer Nachteil von K-means ist dessen erhöhte Sensitivität gegenüber Ausreißern im Datensatz. Weiterentwickelte Modelle wie z.B. **Partitioning Around Medoids** (PAM) Clustering erhöhen die Robustheit eines Modells wie K-means gegenüber Ausreißern.\n",
    "\n",
    "K-means ist einer der ältesten Machine Learning-Algorithmen und stellt anschaulich dar, wie Modelle zu einem Optimum konvergieren können. In dieser Übung haben wir einige Beispiele für die Anwendung von K-means kennengelernt. Die Beispiele, die wir behandelt haben, dienen vor allem der Veranschaulichung, wie wir den Algorithmus in R implementieren können. In der Realität würden wir die Cluster nicht kennen und es würde vor allem darum gehen, sinnvolle Cluster zu **definieren**! Ein Beispiel wäre die Einteilung von Kund:innengruppen (Die/Der Schnäppchenjäger:in, der/die Impulskäufer:in, etc..), oder auch das Clustern in [Sinusmilieus](https://www.sinus-institut.de/sinus-milieus/sinus-milieus-deutschland). Dafür wäre aber ein erheblich größerer Aufwand notwendig, sodass wir uns in dieser Übung nur auf die R-Implementation beschränken."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
