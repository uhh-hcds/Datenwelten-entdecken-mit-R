{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a2a4d6-ea6e-47df-a6d0-2d3dab40e71d",
   "metadata": {},
   "source": [
    "# Naiver Bayes Klassifikator\n",
    "\n",
    "In dieser Übung lernen wir eine Umsetzung des Naive Bayes Classificators aus der Vorlesung kennen. Ein typisches Anwendungsbeispiel für dieses Modell ist die Klassifizierung von Texten, z. B. für die Erkennung von Spam in Textnachrichten. Da die theoretischen Grundlage für Naive Bayes in der Vorlesung behandelt werden, werden wir in dieser Übung nicht detailliert darauf eingehen. Es geht uns eher um eine praktische und robuste Umsetzung. \n",
    "\n",
    "> In dieser Übung möchten wir aber nicht nur den Naive Bayes-Klassifikator anwenden. Wir möchten ihn besser nachvollziehbar machen, indem wir ihn mit eigenen Funktionen selbst aufbauen. Das hat auch den Vorteil, dass ihr dadurch an das Schreiben eigener Funktionen herangeführt werdet. Der Code ist dadurch aber vielleicht etwas komplexer als er sein müsste und auch etwas herausfordernder als bisher gewohnt.\n",
    "\n",
    "Ein typischer Spam-Filter ist in der Lage, Textnachrichten in zwei Kategorien einzusortieren – \"Spam\" und \"Kein Spam\". \"Kein Spam\" wird im Englischen auch oft als \"Ham\" bezeichnet. Daher werden wir für diese Übung mit den zwei Labels `spam` und `ham` arbeiten. Unser Spam-Filter wird mithilfe eines großen Textkorpus trainiert und lernt dadurch, welche Nachrichten `spam` und welche `ham` sind.\n",
    "\n",
    "Wir benutzen wieder die bekannten Pakete `tidyverse` und `tidymodels`. Neu hinzu kommt in dieser Übung zum einen [`tidytext`](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html), welches uns umfangreiche Funktionen für Texte und Wörter bereitstellt, sowie [`patchwork`](https://patchwork.data-imaginist.com/) und [`ggwordcloud`](https://lepennec.github.io/ggwordcloud/) für die grafische Darstellung der Datenexploration. Für `tidytext` gibt es ähnlich wie für `tidyverse` und `tidymodels` ein eigenes [Buch](https://www.tidytextmining.com/) aus dem posit-Umfeld.\n",
    "\n",
    "> Da wir in diesem Übungsblatt mit einigen Zufallsfunktion arbeiten, werden wir am Anfang einmal mithilfe von `set.seed()` den Zufallsgenerator von R an einer bestimmten Stelle starten. Dadurch bleibt der Code reproduzierbar.\n",
    "\n",
    "Der Einfachheit halber werden wir, anders als in Übung 8, ohne das Paket `recipe` arbeiten. Tendenziell ist die Anwendung von `recipe` aber empfehlenswert und mit ein bisschen Übung auch übersichtlicher als ohne. Wir benutzen `recipe` dafür nochmal in Übung 9 & 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c31fd9-76bc-4c4d-974e-b456a799ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman::p_load(tidyverse, tidymodels, tidytext, patchwork, ggwordcloud)\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "# Make plots wider \n",
    "options(repr.plot.width = 18, repr.plot.height = 8) # außerdem wollen wir eine breitere Darstellung der Plots haben!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7d526-39ae-4612-b5c6-4e2bdfe3cad8",
   "metadata": {},
   "source": [
    "## Daten zusammentragen\n",
    "\n",
    "Wir arbeiten mit der [SMS Spam Collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) von Tiado Almeida und Jos Hidalgo. Der Datensatz enthält 5574 SMS-Nachrichten aus verschiedenen Quellen und unterteilt diese in `ham` und `spam`. `ham` sind authentische Nachrichten von echten Personen, und `spam` sind Nachrichten, die automatisiert zu Werbe- oder anderen Zwecken verschickt werden. \n",
    "Alle SMS-Nachrichten mussten zunächst von Menschen \"manuell\" als `ham` oder `spam` eingeordnet werden. Wir haben das Glück, dass wir uns diesen Schritt sparen können, und direkt den bereits gesammelten Datensatz für das Training unseres Modells verwenden können!\n",
    "\n",
    "> Almeida,Tiago and Hidalgo, Jos. (2012). SMS Spam Collection. UCI Machine Learning Repository. https://doi.org/10.24432/C5CC84 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e3b76-9bf5-4b8e-96e8-ff0d4b4e5d6b",
   "metadata": {},
   "source": [
    "## Daten explorieren und vorbereiten\n",
    "\n",
    "Zunächst können wir die SMS spam collection **laden**. Wir benennen die Spalten direkt nach `label` und `msg` und entfernen alle `NA` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531235e8-6c82-4ba5-9e2c-3181c146e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms <- read_delim(\n",
    "    \"data/SMSSpamCollection\", # path to file\n",
    "    col_names = c(\"label\", \"msg\"), # we can directly label the columns\n",
    "    show_col_types = FALSE # suppress warning\n",
    "    ) %>%\n",
    "    drop_na() %>% # remove NA columns\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af65672-6674-434d-9fd4-c65c6e8289b0",
   "metadata": {},
   "source": [
    "Danach bleiben uns 4773 vollständige SMS-Nachrichten. In `label` ist die Entscheidung gespeichert, die unser Modell vorhersagen soll. Als Basis dafür dient der Text der SMS, der in `msg` gespeichert ist.\n",
    "\n",
    "1. Wie viel `ham` und wieviel `spam` ist im Datensatz vorhanden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd0bbb-39d8-4c46-9860-4ac99e26d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf00035-a024-481d-85c9-1f3b0fc1c779",
   "metadata": {},
   "source": [
    "Fast **14 Prozent** der Nachrichten in diesem Datensatz sind also Spam!\n",
    "\n",
    "Mithilfe von `slice_sample(n = n)` können wir uns zufällige Nachrichten anzeigen lassen, um ein **allgemeines Gefühl** über den Datenkorpus zu bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937a1c3-c908-47f0-8ac1-909f214a1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms %>% slice_sample(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3688b3-5b00-4259-8d49-8cda3d20a6e7",
   "metadata": {},
   "source": [
    "Wie wir sehen können, sind viele Nachrichten unaufgeräumt, inkonsistent in Groß- und Kleinschreibung oder beinhalten Zeichenketten, die keine Buchstaben sind. Für unser Naive Bayes Classificator Modell brauchen wir allerdings ein aufgeräumtes Vokabular. Daher müssen wir als nächstes den Datensatz durch **Preprocessing** säubern. Dafür schreiben wir eine Funktion, welche einen Text übergeben bekommt und anschließend eine **\"saubere\"** Version zurückgibt.\n",
    "\n",
    "Schritte des Preprocessing, die wir anwenden wollen:\n",
    "\n",
    "* Zeichensetzung (bis auf den Unterstrich) wird entfernt\n",
    "* Alles wird klein geschrieben\n",
    "* URLs (Webseiten) werden durch `_url_` ersetzt\n",
    "* Lange Zahlenfolgen (meistens Telefonnummern) werden durch `_longnum_` ersetzt\n",
    "* Text, der aus mehreren Wörtern besteht, wird durch `str_split()` auf die einzelnen Wörter aufgeteilt\n",
    "* Alles was übrig ist und aus einem oder weniger Zeichen besteht, wird entfernt.\n",
    "\n",
    "Damit sollten wir die meisten Inkonsistenzen abgedeckt haben!\n",
    "\n",
    "2. Überlegt euch im Kopf, wie ihr eine solche Funktion strukturell aufbauen würdet.\n",
    "\n",
    "<img src=\"https://stringr.tidyverse.org/logo.png\" alt=\"stringr\" width=\"100\" align=\"right\" />Das Ganze sieht dann folgendermaßen aus. Die einzelnen Funktionen wie `str_replace_all` oder `str_to_lower()` sind Teil des Paketes [`stringr`](https://stringr.tidyverse.org/index.html). In der Dokumentation von `stringr` könnt ihr die einzelnen Funktionen genauer nachschlagen. Ihr müsst an dieser Stelle nicht jede Funktion verstehen. Wir haben leider keine Zeit, ausführlich auf die sogenannten [**regular expressions**](https://stringr.tidyverse.org/articles/regular-expressions.html) einzugehen. Mehr zum Thema **regular expressions** findet ihr z.B. in [R4DS](https://r4ds.hadley.nz/regexps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06086bb4-0dcc-467e-b4b5-5cc78b72e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cleaner <- function(text_vector) {\n",
    "    text_vector %>%\n",
    "        # Remove all punctuation except for underscores (since URLs are recoded to _url_)\n",
    "        str_replace_all(\"[^[:alnum:]_ ]+\", \"\") %>%\n",
    "        \n",
    "        # Make everything lower case\n",
    "        str_to_lower() %>%\n",
    "        \n",
    "        # Recode things that look like URLs to the string _url_\n",
    "        str_replace_all(\"\\\\b(http|www\\\\S+)\\\\b\", \"_url_\") %>%\n",
    "        \n",
    "        # Recode long sequences of numbers (e.g., phone numbers) to _longnum_\n",
    "        str_replace_all(\"\\\\b(\\\\d{7,})\\\\b\", \"_longnum_\") %>%\n",
    "        \n",
    "        # Split on spaces\n",
    "        str_split(\" \") %>%\n",
    "        \n",
    "        # Use map to apply the function to each element of the list\n",
    "        map(~ .x[.x != \"\" & nchar(.x) > 1]) # Remove empty strings and strings with 1 or fewer characters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1d61c-dd3e-40ce-a90f-35e19e3cb450",
   "metadata": {},
   "source": [
    "3. Wie würdet ihr die definierte Funktion mit einer ausgedachten Beispielnachricht testen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99de4c-fc7e-4820-84d7-a6ac4882113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cleaner(\"Blabla ...132 hallo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2b7bb-9430-4901-b7f8-5664f7f4ae9b",
   "metadata": {},
   "source": [
    "Ganz praktisch. Jetzt können wir unseren gesamten Datensatz mithilfe der neuen Funktion `string_cleaner()` säubern! Die gesäuberte Nachricht wird in der neuen Spalte `msg_list` als Liste gespeichert. Um uns den `string_cleaner()` kurz genauer anzuschauen, können wir ein neues Datenobjekt `sms_clean` erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cd599-0a35-4496-b7c3-9b8c6d4d718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_clean <- sms %>%\n",
    "    mutate(msg_list = map(msg, string_cleaner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46a56b-feaa-4719-b65b-d09dc3d3711c",
   "metadata": {},
   "source": [
    "4. Lasst euch mithilfe von `slice_sample(n = n)` wieder ein paar Beispiele von `sms_clean` anzeigen anschauen. Welche Spalten enthält unser neue Datensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29218ed-d153-469b-9127-82d1e446e8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4824989a-a056-468e-897a-46719461c7c9",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "Wie in den vorherigen Übungen, teilen wir jetzt den Datensatz wieder in Trainings- und Testdaten auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ba400-9668-4b39-9920-ae6210d738a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split <- sms %>% \n",
    "    initial_split(strata = label) # random split in 75% test & 25% training data\n",
    "\n",
    "train_spam <- split %>%\n",
    "    training()\n",
    "\n",
    "test_spam <- split %>%\n",
    "    testing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b420e-9e15-468b-acdb-a92dd38d6bbf",
   "metadata": {},
   "source": [
    "Die Testdaten lassen wir zunächst unberührt, da wir sie in einem real-world-scenario ja ebenfalls nicht kennen würden. Wir können aber die Trainingsdaten mit unserem `string_cleaner()` säubern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c907b36-2096-4543-a9f1-65838c19bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spam <- train_spam %>%\n",
    "    mutate(msg_list = string_cleaner(.$msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c659875-1e9e-47bf-b834-a332400132ac",
   "metadata": {},
   "source": [
    "Im nächsten Schritt müssen wir ein **Vokabular** erstellen. Das ist ein \"Wörterbuch\" mit allen Worten, die im Datensatz vorkommen. Zusätzlich wird noch gezählt, wie häufig die Worte jeweils vorkommen. Wir brauchen das Vokabular später für die Berechnung der Wahrscheinlichkeiten für den Naive Bayes Klassifikator. Zunächst können wir mithilfe des Vokabulars aber auch die Trainingsdaten visualisieren!\n",
    "\n",
    "5. Überlegt kurz, welche Möglichkeiten euch zur Visualisierung eines Vokabulars einfallen würden.\n",
    "\n",
    "### Vokabular erstellen\n",
    "\n",
    "Um das Vokabular zu erstellen, müssen wir alle einzelnen Wörter aus der Spalte `msg_list` extrahieren und anschließend zählen. Da die einzelnen Elemente von `msg_list` als Listen vorliegen, brauchen wir dafür ein paar mehr Schritte als gewohnt. Das Ergebnis ist ein neuer Datensatz `vocab` mit den Spalten `word` und `n` (für die Worthäufigkeit).\n",
    "\n",
    "> Keine Sorge, der folgende Code mag etwas unübersichtlich erscheinen. Wenn das gerade zu komplex ist, könnt ihr ihn auch einfach ausführen und dann mit dem Ergebnis weiterarbeiten, ohne jeden einzelnen Schritt nachvollziehen zu müssen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064277f-e55c-40a4-bdc4-8c444f97423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab <- train_spam %>%\n",
    "    select(msg_list) %>% # wir wählen lediglich die Spalte msg_list aus\n",
    "    deframe() %>% # in den nächsten drei Schritten machen wir aus der Listenstruktur eine Tabelle\n",
    "    unlist() %>%\n",
    "    table() %>% # durch table() zählen wir die Häufigkeit der einzelnen Wörter\n",
    "    as_tibble() %>% # und konvertieren diese anschließend wieder in einen Tibble\n",
    "    rename(word = '.') %>% # damit es etwas schöner ist, können wir die Spalte . noch in word umbenennen\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509e7af-16c6-4308-8ea8-d819f2760535",
   "metadata": {},
   "source": [
    "### Vokabular erweitern\n",
    "\n",
    "Im nächsten Schritt müssen wir unser vorhin erzeugtes Vokabular `vocab` erweitern. Uns interessiert ja eigentlich die Aufteilung in `ham` und `spam`, daher müssen wir zwei extra Vokabulare `vocab_ham` und `vocab_spam` erstellen! Da wir diesen Schritt zweimal gehen müssen, wollen wir eine eigene Funktion dafür schreiben, denn so sparen wir Code und können die Funktion immer wieder verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e88ad-c304-42df-acdf-859cf0e63448",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vocab <- function(data, label) {\n",
    "    data %>%\n",
    "        filter(label == !!label) %>%\n",
    "        select(msg_list) %>%\n",
    "        deframe() %>%\n",
    "        unlist()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948bd90-d19d-4f3f-baff-166e9d4f4fd0",
   "metadata": {},
   "source": [
    "6. Geht einmal Schritt für Schritt durch, was diese Funktion macht, und erläutert, was in den einzelnen Schritten passiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdf189-06e6-4983-91b8-2ae7aba3d1f1",
   "metadata": {},
   "source": [
    "Diese neue Funktion `create_vocab()` wenden wir jetzt zweimal an, und zwar einmal auf das Label `ham` und auf das Label `spam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc0db2-aabd-42ff-a388-c938905f38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ham <- create_vocab(train_spam, \"ham\")\n",
    "vocab_spam <- create_vocab(train_spam, \"spam\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89882e16-3086-478b-b9c7-6ed8972e06f5",
   "metadata": {},
   "source": [
    "Wir können jetzt die neu erzeugten Vokabulare `vocab_ham` und `vocab_spam` zu unserem allgemeinen Vokabular `vocab` hinzufügen. Für Wörter, die bei einem der beiden Label nicht auftauchen, wird standardmäßig ein `NA` eingetragen. Das `NA` ersetzen wir direkt mit `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0328dfa-caa8-4061-abb1-d0ba972cbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab <- table(vocab_ham) %>%\n",
    "    as_tibble() %>%\n",
    "    rename(n_ham = n) %>%\n",
    "    left_join(vocab, ., by = c(\"word\" = \"vocab_ham\")) # the dot in this line represents the current data objekt of the pipe\n",
    "\n",
    "vocab <- table(vocab_spam) %>%\n",
    "    tibble::as_tibble() %>%\n",
    "    rename(n_spam = n) %>%\n",
    "    left_join(vocab, ., by = c(\"word\" = \"vocab_spam\")) %>%\n",
    "    replace_na(list(n_ham = 0, n_spam = 0))\n",
    "\n",
    "vocab %>% glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6fc10-25bf-485c-8c3b-8282bac2d438",
   "metadata": {},
   "source": [
    "Das wird ja schon etwas übersichtlicher! Der Code fügt dem Vokabular zwei extra Spalten `n_ham` und `n_spam` hinzu. In diesen ist für jedes Wort die Information enthalten, wie häufig es in SMS die als \"ham\" bzw. \"spam\" gelabelt wurden, enthalten ist. Diese Infos brauchen wir später für die Wahrscheinlichkeitsberechnung im Klassifikator. \n",
    "\n",
    "An der Ausgabe oben können wir bspw. sehen, dass `_longnum_\", also lange Zahlenfolgen, sehr viel häufiger in Spam-Nachrichten enthalten sind als in normalen SMS (263 Mal vs 20 Mal). \n",
    "\n",
    "Genaue Informationen über die Anzahl und Wahrscheinlichkeiten von `ham` und `spam` im Trainingsdatensatz brauchen wir für unseren Klassifikator ohnehin. Wir berechnen sie in der nächsten Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b37b5-e554-482a-b9a6-e6cd64cdf367",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_n <- c(\n",
    "    \"unique\" = nrow(vocab),\n",
    "    \"ham\" = length(vocab_ham),\n",
    "    \"spam\" = length(vocab_spam)\n",
    "    ) %>%\n",
    "    print()\n",
    "\n",
    "class_probs <- train_spam %>%\n",
    "    pull(label) %>% # uns interessiert nur die Spalte label\n",
    "    table() %>%\n",
    "    prop.table()\n",
    "\n",
    "class_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683fbc0-bb0c-40fb-aa9d-e82484c63fc9",
   "metadata": {},
   "source": [
    "Tatsächlich, der Anteil an Spam ist auch im Trainingsdatensatz genauso gering wie im Gesamtdatensatz. Die zufällige Aufteilung in Trainings- und Testdaten hat also funktioniert. Die Häufung von langen Zahlenketten in Spam-Nachrichten ist also tatsächlich auffällig und ungewöhnlich. Sie ist jedoch auch leicht erklärbar: Vermutlich sollen die Adressat:innen des Spams dazu verleitet werden, eine bestimmte Telefonnummer anzurufen oder es ist von großen Geldsummen die Rede. Das ist ein guter Plausibilitätscheck für unsere Daten und die Datenaufbereitung!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15357744-493b-4c79-8b82-29634ca0c68b",
   "metadata": {},
   "source": [
    "7. Wieviele einzigartige Wörter sind insgesamt in unserem Vokabular vorhanden?\n",
    "\n",
    "Ganz nebenbei sind wir nach diesen Vorbereitungsarbeiten auch schon relativ nahe an unserem Klassifikator! In einem letzten Schritt müssen wir noch die Wortwahrscheinlichkeiten (abhängig von der Gesamtmenge der Wörter) berechnen. Dafür schreiben wir uns wieder eine eigene Funktion `word_probabilities()`. Diese Funktion setzt die Anzahl eines einzelnen Wortes in Relation mit der gesamten labelspezifischen Anzahl. Außerdem haben wir **Laplacian smoothing** hinzugefügt, damit das Ergebnis niemals 0 wird! Da wir später mit Produkten arbeiten, sollte die Wahrscheinlichkeit nie 0 sein!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9247cb8-950b-44e9-9552-5b9159f50bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_probabilities <- function(n_word, n_category, n_vocab, smooth = 1) {\n",
    "    probability <- (n_word + smooth) / (n_category + smooth * n_vocab)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec4d9b-0662-401e-a63c-fc5ea74d0076",
   "metadata": {},
   "source": [
    "Diese Wahrscheinlichkeitsberechnung wenden wir jetzt auf unser Vokabular `vocab` an und erzeugen zwei weitere Spalten `prob_ham` und `prob_spam`. Dafür benutzen wir die Funktion `rowwise()` aus `dplyr`, um die Berechnung der Wahrscheinlichkeiten pro Zeile anzuwenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72125f12-db4e-45ab-917f-76a45d3e9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab <- vocab %>%\n",
    "    rowwise() %>% # group dataframe rowwise\n",
    "    mutate(\n",
    "        prob_ham = word_probabilities(n_ham, word_n[[\"ham\"]], word_n[[\"unique\"]]),\n",
    "        prob_spam = word_probabilities(n_spam, word_n[[\"spam\"]], word_n[[\"unique\"]])\n",
    "    ) %>%\n",
    "    ungroup() %>% # undo rowwise grouping\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2407b-2bb5-49c2-9c63-cdf9dcd56a53",
   "metadata": {},
   "source": [
    "Perfekt! In den beiden `prob_`-Spalten steht jetzt für jedes Wort eine Wahrscheinlichkeit dafür, dass es in Spam- oder Ham-Nachrichten vorkommt. \n",
    "\n",
    "> Die Zahlen in den Spalten sind hier in R´s wissenschaftlicher Schreibweise dargestellt. e-04 bedeutet beispielsweise \"nimm die Zahl die vor dem e steht mit 0.0001 mal\" (z. B. 4,026074e-04 = 0,0004026074). Je negativer die Zahl hinter dem e, desto kleiner also die Wahrscheinlichkeit des Wortes in der Klasse.\n",
    "\n",
    "Warum sind die Wahrscheinlichkeiten so klein? Wir haben ein speziell auf unsere Trainings-Daten angepasstes Vokabular entwickelt, das für jedes Wort in Bezug auf alle im Datensatz existierenden Worte in der jeweiligen Spalte einen Wert ausgibt und es sind eben viele Worte, auf die die Wahrscheinlichkeiten aufgeteilt werden müssen. In der Summe ergibt sich für jede Spalte der Wert 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e315cc-d768-44d7-85af-4348cf013499",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(vocab$prob_ham) # should be 1\n",
    "sum(vocab$prob_spam) # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6608da-c51b-4c65-83e9-0980cd0b4bf7",
   "metadata": {},
   "source": [
    "Wir haben jetzt ein nach `ham` und `spam` gelabeltes Vokabular mit Häufigkeiten und Wahrscheinlichkeiten erstellt. Damit können wir zum nächsten Schritt übergehen, und zwar einen Naive Bayes Klassifikator zu erstellen und zu trainieren! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932157c-da3a-42e6-a9b1-09d17fea8a82",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #efe3fd\"><h1>Präsenzteil</h1></div> \n",
    "\n",
    "## Daten explorieren und vorbereiten\n",
    "\n",
    "### Explorative Statistik\n",
    "\n",
    "Bevor wir jetzt mit den Naive Bayes Klassifikator weitermachen – wir haben die explorative Statistik ganz vergessen!\n",
    "\n",
    "Daher wollen wir an dieser Stelle nochmal ein paar praktische Visualisierungen erstellen. Dafür eignen sich z. B. ein Histogramm über die Worthäufigkeiten und eine sogenannte Wordcloud. \n",
    "\n",
    "Mithilfe von `(p1 | p2)` können wir die beiden Plots nebeneinander darstellen. Das ermöglicht uns das Paket `patchwork`, von dem weiter oben bereits die Rede war."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f37f8-9cac-4d74-b505-100dd7557525",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 <- vocab %>%\n",
    "    arrange(desc(n)) %>%  # Sort by frequency in descending order\n",
    "    top_n(20, n) %>%\n",
    "ggplot(aes(x = reorder(word, n), y = n)) + \n",
    "    geom_col() +  # geom_col is used for creating bar plots\n",
    "    coord_flip() +  # Flip coordinates to have words on the y-axis\n",
    "    labs(x = \"Word\", y = \"Frequency\", title = \"Top 20 Most Frequent Words\") +\n",
    "    theme_minimal(base_size = 25)  # Use a minimal theme for a clean look\n",
    "\n",
    "p2 <- vocab %>%\n",
    "    top_n(30, n) %>%\n",
    "    ggplot(aes(label = word, size = n, color = n)) +\n",
    "    geom_text_wordcloud(area_corr = 0.5, rm_outside = FALSE) +\n",
    "    scale_size_area(max_size = 30) +  # 'max_size' can increase the size of the largest word\n",
    "    scale_color_gradient(low = \"blue\", high = \"red\") +  # Use a color gradient from blue to red\n",
    "    theme_void()  # This removes axes, backgrounds, etc.\n",
    "\n",
    "(p1 | p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c589c-fa31-4efd-9eda-6bb4db5bcf1b",
   "metadata": {},
   "source": [
    "Die häufigsten Wörter sind solche wie \"to\", \"you\", \"the\", \"and\", usw. Das sind sogenannte **stopwords**. Für eine aussagekräftigere Analyse könnten wir diese noch \"aussortieren\", also aus den Daten herausfiltern. Dafür können wir die Tabelle `stop_words` aus dem Paket `tidytext` benutzen. Keine Sorge, das dient hier nur der Vollständigkeit – es ist vielleicht hilfreich, wenn ihr so etwas schon mal gesehen habt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a6ddc-bef9-4cd5-9573-d5a25d088ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data(stop_words)\n",
    "\n",
    "p1 <- vocab %>%\n",
    "    anti_join(stop_words, by = \"word\") %>%\n",
    "    arrange(desc(n)) %>%  # Sort by frequency in descending order\n",
    "    top_n(20, n) %>%\n",
    "ggplot(aes(x = reorder(word, n), y = n)) + \n",
    "    geom_col() +  # geom_col is used for creating bar plots\n",
    "    coord_flip() +  # Flip coordinates to have words on the y-axis\n",
    "    labs(x = \"Word\", y = \"Frequency\", title = \"Top 20 Most Frequent Words\") +\n",
    "    theme_minimal(base_size = 25)  # Use a minimal theme for a clean look\n",
    "\n",
    "p2 <- vocab %>%\n",
    "    anti_join(stop_words, by = \"word\") %>%\n",
    "    top_n(15, n) %>%\n",
    "    ggplot(aes(label = word, size = n, color = n)) +\n",
    "    geom_text_wordcloud(area_corr = 0.5, rm_outside = FALSE) +\n",
    "    scale_size_area(max_size = 30) +  # 'max_size' can increase the size of the largest word\n",
    "    scale_color_gradient(low = \"blue\", high = \"red\") +  # Use a color gradient from blue to red\n",
    "    theme_void()  # This removes axes, backgrounds, etc.\n",
    "\n",
    "(p1 | p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e98fd9-6e41-4958-825b-edb3976a381c",
   "metadata": {},
   "source": [
    "8. Vergleicht Version 1 und Version 2 (ohne stopwords) miteinander."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cc41d-a86d-4372-ad0a-4c032eb3aa71",
   "metadata": {},
   "source": [
    "9. Wie sieht das ganze aus, wenn ihr den gleichen Graphen getrennt für die Labels `ham` und `spam` erstellt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe3b45-414a-4817-98e2-4481256d74a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365b93c2-d5b0-4742-8a90-03b6bacfbe39",
   "metadata": {},
   "source": [
    "## Modell trainieren\n",
    "\n",
    "Jetzt geht es los! Im nächsten Schritt schreiben wir eine neue Funktion `classifier()`, welche folgende Argumente annimmt:\n",
    "\n",
    "* `msg`: Die Textnachricht, die klassifiziert werden soll\n",
    "* `prob_df`: Ein Dataframe mit Wahrscheinlichkeiten für einzelne Wörter (also in unserem Fall das oben erzeugte Vokabular `vocab`)\n",
    "* `p_ham = 0.5`: Baseline Wahrscheinlichkeit für `ham`\n",
    "* `p_spam = 0.5`: Baseline Wahrscheinlichkeit für `spam`\n",
    "\n",
    "> Kurzer Disclaimer: wir schreiben im Folgenden unser Modell als eigene Funktion. Bisher haben wir ja immer vorprogrammierte Modelle aus Paketen genommen. Auch `naiveBayes()` gibt es als Modell, z. B. aus dem Paket `e1071`. Falls euch dieser Schritt hier viel zu kompliziert ist, könnt ihr ihn auch überspringen und die Funktion als Blackbox betrachten, bei der ihr zwar nicht genau wisst, wie sie funktioniert, aber annehmen könnt, dass sie funktioniert! Und falls ihr interessiert seid, versucht gerne die einzelnen Schritte der Funktion nachzuvollziehen!\n",
    "\n",
    "Dann werden die folgenden Schritte durchlaufen:\n",
    "\n",
    "1. Der `classifier()` säubert die `msg` zunächst mit unser Hilfsfunktion `string_cleaner()` und erzeugt dann einzelne Elemente daraus.\n",
    "2. Für jedes Element aus der gesäuberten Nachricht wird im Vokabular `vocab` nachgeschlagen, welche Wahrscheinlichkeiten für dieses Wort für `ham` und `spam` vorliegen.\n",
    "3. Wenn das Wort nicht im Vokabular vorkommt, wird als Klassifizierung `unknown` ausgewählt. Ansonsten folgt Schritt 4.\n",
    "4. Als nächstes wird ein `tibble` erzeugt, welcher für jedes Wort der Nachricht `msg` eine eigene Zeile enthält. In jeder Zeile stehen die Wahrscheinlichkeiten, dass das Wort im `spam` oder `ham` Vokabular auftaucht.\n",
    "5. In diesem Schritt werden alle Wahrscheinlichkeiten pro `spam` und `ham` multipliziert, um die Gesamtwahrscheinlichkeit der Nachricht zu ermitteln. Dies entspricht dem ersten Teil der Näherung aus der Vorlesung, indem alle einzelnen Wortwahrscheinlichkeiten miteinander multipliziert werden.\n",
    "6. Wir multiplizieren die berechneten Wahrscheinlichkeiten nochmal mit den Gewichtungen `p_ham` und `p_spam`. Das entspricht dem zweiten Teil der Näherung aus der Vorlesung, bei dem die Wortwahrscheinlichkeit als Anteil aus dem gesamten Wortschatz dazu multipliziert wird.\n",
    "7. Wenn die Wahrscheinlichkeit für das Label `spam` größer ist als die für `ham`, wird die Nachricht als `spam` klassifiziert. Ist sie kleiner klassifiziert das Modell `ham`.\n",
    "8. Letzter Schritt: die berechnete Klassifizierung wird zurückgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9fe91-a349-45d3-a091-37115f465c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines a function called 'classifier' that takes in four parameters:\n",
    "# 'msg' is the message to classify,\n",
    "# 'prob_df' is a data frame with the probabilities of each word being ham or spam,\n",
    "# 'p_ham' is the prior probability of a message being 'ham' (not spam),\n",
    "# 'p_spam' is the prior probability of a message being 'spam'.\n",
    "classifier <- function(msg, prob_df, p_ham = 0.5, p_spam = 0.5) {\n",
    "    \n",
    "    # Schritt 1: Clean the input message using 'string_cleaner()' and then unlist the result.\n",
    "    clean_message <- string_cleaner(msg) %>% \n",
    "        unlist()\n",
    "\n",
    "    # Schritt 2: For each word in the cleaned message, look up the probabilities of\n",
    "    # that word being ham or spam from the 'prob_df' data frame. It uses 'sapply' to \n",
    "    # apply the function to each word, 'filter' to select the rows where the word \n",
    "    # matches, and 'select' to keep only the columns with ham and spam probabilities.\n",
    "    probs <- sapply(clean_message, function(x) {\n",
    "        filter(prob_df, word == x) %>%\n",
    "        select(prob_ham, prob_spam)\n",
    "        })\n",
    "\n",
    "    # Schritt 3: Check if the 'probs' object exists and has dimensions. If it doesn't,\n",
    "    # the message is classified as 'unknown'. No further calculation needed.\n",
    "    if (is.null(dim(probs))) {\n",
    "       classification <- \"unknown\"\n",
    "       return(classification)\n",
    "    }\n",
    "\n",
    "    # Schritt 4: Convert the matrix of probabilities into a tibble with one column for ham and one for spam. \n",
    "    # Each row corresponds to the probabilities for each word in the message.\n",
    "    classification <- tibble(\n",
    "         ham = as.numeric(probs[1, ]),\n",
    "         spam = as.numeric(probs[2, ])) %>%\n",
    "\n",
    "    # Schritt 5: Multiply all the ham probabilities together, and all the spam probabilities \n",
    "    # together, to get the overall probability of the message being ham or spam.\n",
    "    summarise(across(everything(), ~prod(.x, na.rm = TRUE))) %>%\n",
    "\n",
    "    # Schritt 6: Adjust these probabilities by the prior probabilities of any message being ham or spam.\n",
    "    mutate(\n",
    "        ham = ham * p_ham,\n",
    "        spam = spam * p_spam) %>%\n",
    "\n",
    "    # Schritt 7: Summarise the results into a single classification. \n",
    "    # If the probability of ham is higher, classify as 'ham'.\n",
    "    # If spam is higher, classify as 'spam'.\n",
    "    # If they are equal, it's 'unknown'.\n",
    "    summarise(classification = case_when(\n",
    "        spam <= ham ~ \"ham\",\n",
    "        spam >  ham ~ \"spam\",\n",
    "        TRUE ~ \"unknown\"\n",
    "    )) %>%\n",
    "\n",
    "    # Pull the classification from the tibble.\n",
    "    pull(classification)\n",
    "    \n",
    "    # Schritt 8: Return the calculated classification\n",
    "    return(classification)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15a63c-fe4c-468e-85c6-82c4e3fc7b0e",
   "metadata": {},
   "source": [
    "## Modell evaluieren\n",
    "\n",
    "<img src=\"https://yardstick.tidymodels.org/logo.png\" alt=\"yardstick\" width=\"100\" align=\"right\" />\n",
    "\n",
    "Diese Klassifikatorfunktion `classifier()` können wir jetzt auf unseren Testdatensatz `test_spam` anwenden, um die Performance des Klassifikators zu evaluieren. Dafür wenden wir die Funktion `classifier()` auf jedes Element der Spalte `msg` in `test_spam` an, und zwar mithilfe von `map()`.\n",
    "\n",
    "> Achtung, das kann gut eine Minute dauern, da die Funktion für relative viele Nachrichten durchgerechnet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a213763-d78c-424a-ab0e-1effb614882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classification <- test_spam %>%\n",
    "    pull(msg) %>%\n",
    "    map(~ classifier(.x, vocab, class_probs[[\"ham\"]], class_probs[[\"spam\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088311e3-458c-434d-b60b-492ceba41bd8",
   "metadata": {},
   "source": [
    "Wir haben jetzt den kompletten Testdatensatz klassifiziert und können die vorhergesagten Labels mit den ursprünglichen Labels. Dafür fügen wir die vorhergesagten Werte aus `spam_classification` einfach als neue Spalte zum Testdatensatz `test_spam` dazu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d971d-ba45-4670-ac03-4d3b6752448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels <- c(\"ham\", \"spam\", \"unknown\")\n",
    "\n",
    "results <- test_spam %>%\n",
    "    mutate(\n",
    "        label = factor(.$label, levels = labels),\n",
    "        predicted = factor(spam_classification, levels = labels)) %>%\n",
    "    glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c03bcd-bf87-4181-8b10-e7b59e5df31d",
   "metadata": {},
   "source": [
    "Jetzt können wir wie gewohnt unsere Labels `label` mit der Vorhersage `predicted` vergleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7ff3a-6196-4f98-9fbe-fde8f7b19342",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Für einen schnellen Überblick können wir wie in den Übungen zuvor eine **Confusion Matrix** erstellen. Dafür können wir wieder die Funktion `conf_mat()` aus dem Paket `yardstick` benutzen, welches wir ebenfalls mit `tidymodels` geladen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b666f5e-b9c9-4e22-8552-4c4b3b17548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>%\n",
    "    conf_mat(truth = label, estimate = predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106ead5-1ae1-4190-ba76-e4cc4adf679e",
   "metadata": {},
   "source": [
    "10. Wieviele Nachrichten wurden richtig vorhergesagt? Wieviele falsch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b023d-228b-4faf-97de-cc314d1a6018",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Wir können jetzt wieder die Accuracy berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997f4e9-91ef-4706-82e3-e0feab45a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics <- metric_set(accuracy)\n",
    "\n",
    "results %>%\n",
    "    metrics(truth = label, estimate = predicted) %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a686501-08a4-4a5c-b300-5f0452b6e216",
   "metadata": {},
   "source": [
    "Unser Klassifikator kommt fast auf 100%! \n",
    "\n",
    "### Vergleich mit \"einfachem\" Modell\n",
    "\n",
    "Wir können das vergleichen mit dem \"einfachsten\" Modell, was wir uns vorstellen können – einfach alles als `ham` zu labeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f3f1c-6d99-486a-ad01-706228bfc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results %>%    \n",
    "    mutate(all_ham = \"ham\") %>% \n",
    "    mutate(all_ham = factor(all_ham, levels = labels)) %>% \n",
    "    metrics(truth = label, estimate = all_ham) %>%\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90972a50-20ea-4105-b2bc-0d03c039b14e",
   "metadata": {},
   "source": [
    "Es liegt auf der Hand, dass dabei einfach die Häufigkeit des Labels `ham` im Testdatensatz bei rauskommt, etwa 86%, denn in den jestlichen knapp 14 Prozent der fälle macht dieses \"Pauschal-Modell\" ja Fehler, weil es den Spam einfach immer übersieht. \n",
    "\n",
    "Wir erreichen also durch unseren gerade gebauten Spam-Filter eine wesentliche Verbesserung von 86% auf 98%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748b079-ffec-4ab1-9b4a-47aa744a0e80",
   "metadata": {},
   "source": [
    "## Modell verbessern\n",
    "\n",
    "Diese Übung war schon recht lang und teilweise ganz schön kompliziert! Daher werden wir an dieser Stelle nicht detailliert darauf eingehen, wie wir das Modell noch verbessern könnten. Trotzdem seien einige Punkte gesagt:\n",
    "\n",
    "* Wir erinnern uns, dass wir an einer Stelle weiter oben den Laplacian `smooth` gesetzt hatten. Mit diesem Wert können wir etwas herumspielen und schauen, ob sich das Ergebnis verbessert.\n",
    "* Außerdem könnte ein ausgefeilteres Preprocessing helfen, also eine bessere Vorbereitung der Daten für unsere Analyse. Z. B. könnten wir vorab Wörter mit extrem geringen Wahrscheinlichkeiten herausfiltern (z. B. welche die insgesamt nur 1-2 mal im  Datensatz auftauchen).\n",
    "* Ganz allgemein gesprochen profitiert ein Spam Filter (wie viele andere Machine Learning Algorithmen) von einer verbesserten Datengrundlage, sprich einer größeren Menge an gelabelten Trainingsdaten. Unser Filter trainiert lediglich auf ein paar tausend Textnachrichten, was natürlich nicht besonders viel ist!\n",
    "\n",
    "Interessant wäre auch ein Vergleich von unserer eigens geschriebenen Funktion und Naive Bayes-Implementationen aus anderen Paketen, wie z. B. `naiveBayes()` aus dem Paket `e1071`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319cb4ec-ec47-413b-b753-a74caedbb150",
   "metadata": {},
   "source": [
    "## Real-world scenario\n",
    "\n",
    "Wie können wir uns den Spam Filter denn jetzt eigentlich zunutze machen für eine neue Nachricht, die wir erhalten?\n",
    "\n",
    "Ganz einfach, wir haben ja unseren Klassifikator `classifier()` und unsere Datengrundlage über das Vokabular `vocab`. Wir können also neue Nachrichten, z. B. `new_msg`, einfach unserem Klassifikator übergeben und dieser gibt uns dann 'ham' oder 'spam' zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5b883-c7fd-4e18-b251-0eae4a8e9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can I now check new messages?\n",
    "new_msg = \"many chances to win free cash! dial 017623448234\"\n",
    "classifier(new_msg, vocab, class_probs[[\"ham\"]], class_probs[[\"spam\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143fff4-dba8-4732-a39f-21e029fbcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg = \"hey, i will be late, don't wait for dinner!\"\n",
    "classifier(new_msg, vocab, class_probs[[\"ham\"]], class_probs[[\"spam\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb749a-d483-47f0-83f1-791462b27555",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "In dieser Übung haben wir einen ersten Einblick in die Arbeit mit Textdaten bekommen und haben einen einfachen Spam Filter implementiert. Dieser funktioniert auf unseren Testdaten erstaunlich gut (fast 100% Genauigkeit) und schafft es, den Spam von den authentischen Nachrichten zu trennen! Teilweise wurde es ganz schön kompliziert, da wir den eigentlichen Klassifikator des Modells als eigene Funktion `classifier()` geschrieben haben! Das ist etwas Neues, da wir bisher immer bereits vorgeschriebene Modelle genutzt haben. Auch beim Spam Filter haben wir den klassischen Ablauf der Datenvorbereitung, des Trainings und der Evaluation genutzt.\n",
    "\n",
    "Wir werden im Laufe der Übungen nochmal auf Textverarbeitung eingehen, was ein sehr spannendes Feld des maschinellen Lernens ist. Generative Künstliche Intelligenzen wie ChatGPT sind letztendlich auch aus Fragestellungen des maschinellen Lernens im Bereich der Textverarbeitung entstanden, auch wenn sie natürlich extrem komplexe Machine Learning Algorithmen mit einer riesigen Datengrundlage sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
